{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59216bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/tammy.kojima/Authorship-attribution/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfe\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/Authorship-attribution/feature_extraction.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtweetnlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NER\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# basic loader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, load_dataset, load_trainer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model class\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_classification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Classifier, Sentiment, Offensive, Emoji, Emotion, Irony, Hate,\\\n\u001b[1;32m      6\u001b[0m     TopicClassification, StanceAtheism, StanceAbortion, StanceClimate, StanceHillary, StanceFeminist\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/loader.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_classification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sentiment, Offensive, Irony, Hate, Emotion, Emoji,\\\n\u001b[1;32m      2\u001b[0m     TopicClassification, StanceAtheism, StanceAbortion, StanceClimate, StanceHillary, StanceFeminist\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_classification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset_text_classification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_classification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerTextClassification\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/text_classification/model.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, get_preprocessor\n\u001b[1;32m     10\u001b[0m DEFAULT_CACHE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/.cache/tweetnlp/classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m MODEL_LIST \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-roberta-base-emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     }\n\u001b[1;32m     51\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/util.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murlextract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m URLExtract\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequence, ClassLabel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoConfig,\\\n\u001b[1;32m      9\u001b[0m     AutoModelForMaskedLM\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/datasets/arrow_dataset.py:90\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m     FeatureType,\n\u001b[1;32m     82\u001b[0m     _align_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     require_decoding,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m     fingerprint_transform,\n\u001b[1;32m     92\u001b[0m     format_kwargs_for_fingerprint,\n\u001b[1;32m     93\u001b[0m     format_transform_for_fingerprint,\n\u001b[1;32m     94\u001b[0m     generate_fingerprint,\n\u001b[1;32m     95\u001b[0m     generate_random_fingerprint,\n\u001b[1;32m     96\u001b[0m     get_temporary_cache_files_directory,\n\u001b[1;32m     97\u001b[0m     is_caching_enabled,\n\u001b[1;32m     98\u001b[0m     maybe_register_dataset_for_temp_dir_deletion,\n\u001b[1;32m     99\u001b[0m     update_fingerprint,\n\u001b[1;32m    100\u001b[0m     validate_fingerprint,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_table, get_format_type_from_alias, get_formatter, query_table\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyDict, _is_range_contiguous\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/datasets/fingerprint.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxxhash\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m INVALID_WINDOWS_CHARACTERS_IN_PATH\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/xxhash/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_xxhash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     xxh32,\n\u001b[1;32m      3\u001b[0m     xxh32_digest,\n\u001b[1;32m      4\u001b[0m     xxh32_intdigest,\n\u001b[1;32m      5\u001b[0m     xxh32_hexdigest,\n\u001b[1;32m      6\u001b[0m     xxh64,\n\u001b[1;32m      7\u001b[0m     xxh64_digest,\n\u001b[1;32m      8\u001b[0m     xxh64_intdigest,\n\u001b[1;32m      9\u001b[0m     xxh64_hexdigest,\n\u001b[1;32m     10\u001b[0m     xxh3_64,\n\u001b[1;32m     11\u001b[0m     xxh3_64_digest,\n\u001b[1;32m     12\u001b[0m     xxh3_64_intdigest,\n\u001b[1;32m     13\u001b[0m     xxh3_64_hexdigest,\n\u001b[1;32m     14\u001b[0m     xxh3_128,\n\u001b[1;32m     15\u001b[0m     xxh3_128_digest,\n\u001b[1;32m     16\u001b[0m     xxh3_128_intdigest,\n\u001b[1;32m     17\u001b[0m     xxh3_128_hexdigest,\n\u001b[1;32m     18\u001b[0m     XXHASH_VERSION,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VERSION, VERSION_TUPLE\n\u001b[1;32m     24\u001b[0m xxh128 \u001b[38;5;241m=\u001b[39m xxh3_128\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/tammy.kojima/Authorship-attribution/')\n",
    "import feature_extraction as fe\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12632210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>@KraziiKayy Speaking of Anna we got a 95 on th...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>@garyshort I somehow feel you were at a distin...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>@joshtastic1 Oh I dunno,,I couldnt explain thi...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Yeah T just go to http://www.youravon.com/sall...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Gah! My eyes are getting swimmy! &amp;gt; Better g...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text origin     model  \\\n",
       "0      Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1      ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2      ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3      [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4      [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "...                                                  ...    ...       ...   \n",
       "19995  @KraziiKayy Speaking of Anna we got a 95 on th...  human       NaN   \n",
       "19996  @garyshort I somehow feel you were at a distin...  human       NaN   \n",
       "19997  @joshtastic1 Oh I dunno,,I couldnt explain thi...  human       NaN   \n",
       "19998  Yeah T just go to http://www.youravon.com/sall...  human       NaN   \n",
       "19999  Gah! My eyes are getting swimmy! &gt; Better g...  human       NaN   \n",
       "\n",
       "       tweet_id topic  length batch_timestamp  characters_remaining  \n",
       "0           NaN   NaN     NaN             NaN                   NaN  \n",
       "1           NaN   NaN     NaN             NaN                   NaN  \n",
       "2           NaN   NaN     NaN             NaN                   NaN  \n",
       "3           NaN   NaN     NaN             NaN                   NaN  \n",
       "4           NaN   NaN     NaN             NaN                   NaN  \n",
       "...         ...   ...     ...             ...                   ...  \n",
       "19995       NaN   NaN     NaN             NaN                   NaN  \n",
       "19996       NaN   NaN     NaN             NaN                   NaN  \n",
       "19997       NaN   NaN     NaN             NaN                   NaN  \n",
       "19998       NaN   NaN     NaN             NaN                   NaN  \n",
       "19999       NaN   NaN     NaN             NaN                   NaN  \n",
       "\n",
       "[20000 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mistral = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_mistral.csv')\n",
    "df_gpt = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt.csv')\n",
    "df_human = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_human.csv')\n",
    "df_mistral = df_mistral.iloc[:5000]\n",
    "df_gpt = df_gpt.iloc[:5000]\n",
    "df_human = df_human.iloc[:10000]\n",
    "df_test = pd.concat([df_gpt, df_mistral, df_human], ignore_index=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d7b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizando textos:   0%|                                                                                                                        | 0/20000 [00:00<?, ?it/s][nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "Tokenizando textos: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4104.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza todos os textos primeiro\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()\n",
    "processed_texts = [fe.preprocess_text(texto) for texto in tqdm(textos, desc=\"Tokenizando textos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc89f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prossessed_texts = pd.DataFrame({'text': processed_texts})\n",
    "df_prossessed_texts.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_tokens_geral.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b46fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['our', 'rules', 'are', 'the', 'most', 'robust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['...', 'associating', 'outsiders', 'with', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['...', 'alarms', 'were', 'raised', 'in', 'the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['[', 'the', 'opposition', ']', 'utilizes', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['[', 'the', \"governor's\", ']', 'fixation', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>['@kraziikayy', 'speaking', 'of', 'anna', 'we'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>['@garyshort', 'i', 'somehow', 'feel', 'you', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>['@joshtastic1', 'oh', 'i', 'dunno', ',', ',',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>['yeah', 't', 'just', 'go', 'to', 'http://www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>['gah', '!', 'my', 'eyes', 'are', 'getting', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      ['our', 'rules', 'are', 'the', 'most', 'robust...\n",
       "1      ['...', 'associating', 'outsiders', 'with', 'd...\n",
       "2      ['...', 'alarms', 'were', 'raised', 'in', 'the...\n",
       "3      ['[', 'the', 'opposition', ']', 'utilizes', 'e...\n",
       "4      ['[', 'the', \"governor's\", ']', 'fixation', 'w...\n",
       "...                                                  ...\n",
       "19995  ['@kraziikayy', 'speaking', 'of', 'anna', 'we'...\n",
       "19996  ['@garyshort', 'i', 'somehow', 'feel', 'you', ...\n",
       "19997  ['@joshtastic1', 'oh', 'i', 'dunno', ',', ',',...\n",
       "19998  ['yeah', 't', 'just', 'go', 'to', 'http://www....\n",
       "19999  ['gah', '!', 'my', 'eyes', 'are', 'getting', '...\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prossessed_texts = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_tokens_geral.csv')\n",
    "df_prossessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e80d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 18275.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   tweet_id topic  length batch_timestamp  characters_remaining  \\\n",
       "0       NaN   NaN     NaN             NaN                   NaN   \n",
       "1       NaN   NaN     NaN             NaN                   NaN   \n",
       "2       NaN   NaN     NaN             NaN                   NaN   \n",
       "3       NaN   NaN     NaN             NaN                   NaN   \n",
       "4       NaN   NaN     NaN             NaN                   NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.818182                  22                    18   \n",
       "1                  0.952381                  21                    20   \n",
       "2                  0.914286                  35                    32   \n",
       "3                  0.760000                  25                    19   \n",
       "4                  0.933333                  15                    14   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 4.318182                      8.216942   \n",
       "1                 5.476190                     11.392290   \n",
       "2                 4.714286                      5.632653   \n",
       "3                 4.880000                      9.785600   \n",
       "4                 5.533333                     10.782222   \n",
       "\n",
       "   lexical_stopword_ratio  \n",
       "0                0.181818  \n",
       "1                0.190476  \n",
       "2                0.228571  \n",
       "3                0.360000  \n",
       "4                0.200000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista para armazenar resultados\n",
    "lexical_features = []\n",
    "\n",
    "for texto in tqdm(textos):\n",
    "    # Tokeniza o texto usando Twokenizer\n",
    "    words = fe.preprocess_text(texto)  # já retorna tokens em lowercase\n",
    "    \n",
    "    # Extrai features léxicas\n",
    "    features = fe.extract_lexical_features(texto, words)\n",
    "    lexical_features.append(features)\n",
    "\n",
    "df_lexical = pd.DataFrame(lexical_features)\n",
    "df_test = pd.concat([df_test, df_lexical], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77887d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estilísticas: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 125719.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>stylistic_question_density</th>\n",
       "      <th>stylistic_ellipsis_count</th>\n",
       "      <th>stylistic_emoji_density</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   tweet_id topic  length batch_timestamp  characters_remaining  \\\n",
       "0       NaN   NaN     NaN             NaN                   NaN   \n",
       "1       NaN   NaN     NaN             NaN                   NaN   \n",
       "2       NaN   NaN     NaN             NaN                   NaN   \n",
       "3       NaN   NaN     NaN             NaN                   NaN   \n",
       "4       NaN   NaN     NaN             NaN                   NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  ...  lexical_avg_word_length  \\\n",
       "0                  0.818182                  22  ...                 4.318182   \n",
       "1                  0.952381                  21  ...                 5.476190   \n",
       "2                  0.914286                  35  ...                 4.714286   \n",
       "3                  0.760000                  25  ...                 4.880000   \n",
       "4                  0.933333                  15  ...                 5.533333   \n",
       "\n",
       "   lexical_word_length_variance  lexical_stopword_ratio  \\\n",
       "0                      8.216942                0.181818   \n",
       "1                     11.392290                0.190476   \n",
       "2                      5.632653                0.228571   \n",
       "3                      9.785600                0.360000   \n",
       "4                     10.782222                0.200000   \n",
       "\n",
       "   stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                         0                         0   \n",
       "1                         1                         0   \n",
       "2                         1                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  stylistic_question_density  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   stylistic_ellipsis_count  stylistic_emoji_density  \\\n",
       "0                         0                      0.0   \n",
       "1                         1                      0.0   \n",
       "2                         1                      0.0   \n",
       "3                         0                      0.0   \n",
       "4                         0                      0.0   \n",
       "\n",
       "   stylistic_emoticon_density  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         0.0  \n",
       "4                         0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylistic_features = [\n",
    "    fe.extract_stylistic_features(textos[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_stylistic = pd.DataFrame(stylistic_features)\n",
    "df_test = pd.concat([df_test, df_stylistic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23897837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estruturais: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 155212.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "      <th>structural_has_url</th>\n",
       "      <th>structural_has_mention</th>\n",
       "      <th>structural_has_hashtag</th>\n",
       "      <th>structural_is_retweet</th>\n",
       "      <th>structural_url_density</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   tweet_id topic  length batch_timestamp  characters_remaining  \\\n",
       "0       NaN   NaN     NaN             NaN                   NaN   \n",
       "1       NaN   NaN     NaN             NaN                   NaN   \n",
       "2       NaN   NaN     NaN             NaN                   NaN   \n",
       "3       NaN   NaN     NaN             NaN                   NaN   \n",
       "4       NaN   NaN     NaN             NaN                   NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  ...  \\\n",
       "0                  0.818182                  22  ...   \n",
       "1                  0.952381                  21  ...   \n",
       "2                  0.914286                  35  ...   \n",
       "3                  0.760000                  25  ...   \n",
       "4                  0.933333                  15  ...   \n",
       "\n",
       "   stylistic_emoticon_density  structural_has_url  structural_has_mention  \\\n",
       "0                         0.0                   0                       0   \n",
       "1                         0.0                   0                       0   \n",
       "2                         0.0                   0                       0   \n",
       "3                         0.0                   0                       0   \n",
       "4                         0.0                   0                       0   \n",
       "\n",
       "   structural_has_hashtag  structural_is_retweet  structural_url_density  \\\n",
       "0                       0                      0                     0.0   \n",
       "1                       0                      0                     0.0   \n",
       "2                       0                      0                     0.0   \n",
       "3                       0                      0                     0.0   \n",
       "4                       0                      0                     0.0   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \n",
       "0                      0.0                          0.0  \n",
       "1                      0.0                          0.0  \n",
       "2                      0.0                          0.0  \n",
       "3                      0.0                          0.0  \n",
       "4                      0.0                          0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_features = [\n",
    "    fe.extract_structural_features(textos[i], processed_texts[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estruturais\")\n",
    "]\n",
    "df_structural = pd.DataFrame(structural_features)\n",
    "df_test = pd.concat([df_test, df_structural], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bbc603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features sintáticas: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [01:11<00:00, 281.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>syntactic_subordinating_conj</th>\n",
       "      <th>syntactic_comma_ratio</th>\n",
       "      <th>syntactic_punct_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.032114</td>\n",
       "      <td>2.780466</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.971333</td>\n",
       "      <td>2.902002</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089713</td>\n",
       "      <td>3.198872</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.817905</td>\n",
       "      <td>2.375897</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>2.523211</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   tweet_id topic  length batch_timestamp  characters_remaining  \\\n",
       "0       NaN   NaN     NaN             NaN                   NaN   \n",
       "1       NaN   NaN     NaN             NaN                   NaN   \n",
       "2       NaN   NaN     NaN             NaN                   NaN   \n",
       "3       NaN   NaN     NaN             NaN                   NaN   \n",
       "4       NaN   NaN     NaN             NaN                   NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  ...  \\\n",
       "0                  0.818182                  22  ...   \n",
       "1                  0.952381                  21  ...   \n",
       "2                  0.914286                  35  ...   \n",
       "3                  0.760000                  25  ...   \n",
       "4                  0.933333                  15  ...   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \\\n",
       "0                      0.0                          0.0   \n",
       "1                      0.0                          0.0   \n",
       "2                      0.0                          0.0   \n",
       "3                      0.0                          0.0   \n",
       "4                      0.0                          0.0   \n",
       "\n",
       "   syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "0                   2.032114                      2.780466   \n",
       "1                   1.971333                      2.902002   \n",
       "2                   2.089713                      3.198872   \n",
       "3                   1.817905                      2.375897   \n",
       "4                   1.808046                      2.523211   \n",
       "\n",
       "   syntactic_avg_sentence_length  syntactic_subordinating_conj  \\\n",
       "0                           17.0                           0.0   \n",
       "1                            9.0                           0.0   \n",
       "2                           32.0                           0.0   \n",
       "3                           22.0                           0.0   \n",
       "4                           13.0                           0.0   \n",
       "\n",
       "   syntactic_comma_ratio  syntactic_punct_ratio  \n",
       "0               0.090909               0.227273  \n",
       "1               0.047619               0.238095  \n",
       "2               0.000000               0.114286  \n",
       "3               0.000000               0.120000  \n",
       "4               0.000000               0.200000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features = [\n",
    "    fe.extract_syntactic_features(tokens)\n",
    "    for tokens in tqdm(processed_texts, desc=\"Extraindo features sintáticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(syntactic_features)\n",
    "df_test = pd.concat([df_test, df_syntactic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ac788c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>adaptive</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>ai</th>\n",
       "      <th>ai can</th>\n",
       "      <th>ai in</th>\n",
       "      <th>ai is</th>\n",
       "      <th>ai is revolutionizing</th>\n",
       "      <th>ai powered</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>with ai</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you know</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  adaptive  after  again  ai  ai can  ai in  ai is  \\\n",
       "0      0         0      0      0   0       0      0      0   \n",
       "1      0         0      0      0   0       0      0      0   \n",
       "2      0         0      0      0   0       0      0      0   \n",
       "3      0         0      0      0   0       0      0      0   \n",
       "4      0         0      0      0   0       0      0      0   \n",
       "\n",
       "   ai is revolutionizing  ai powered  ...  will  with  with ai  work  world  \\\n",
       "0                      0           0  ...     0     0        0     0      0   \n",
       "1                      0           0  ...     0     1        0     0      0   \n",
       "2                      0           0  ...     0     0        0     0      0   \n",
       "3                      0           0  ...     0     0        0     0      0   \n",
       "4                      0           0  ...     0     1        0     0      0   \n",
       "\n",
       "   would  yes  you  you know  your  \n",
       "0      0    0    0         0     0  \n",
       "1      0    0    0         0     0  \n",
       "2      0    0    0         0     0  \n",
       "3      0    0    0         0     0  \n",
       "4      0    0    0         0     0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_df = fe.extract_ngrams_features(processed_texts, max_features=300)\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f71c25be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1093876/3577768971.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>length</th>\n",
       "      <th>batch_timestamp</th>\n",
       "      <th>characters_remaining</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>with ai</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you know</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     1.0  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     1.0  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     1.0  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     1.0  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     1.0  chat gpt   \n",
       "\n",
       "   tweet_id topic  length batch_timestamp  characters_remaining  \\\n",
       "0       NaN   NaN     NaN             NaN                   NaN   \n",
       "1       NaN   NaN     NaN             NaN                   NaN   \n",
       "2       NaN   NaN     NaN             NaN                   NaN   \n",
       "3       NaN   NaN     NaN             NaN                   NaN   \n",
       "4       NaN   NaN     NaN             NaN                   NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  ...  will  with  with ai  \\\n",
       "0                  0.818182                  22  ...     0     0        0   \n",
       "1                  0.952381                  21  ...     0     1        0   \n",
       "2                  0.914286                  35  ...     0     0        0   \n",
       "3                  0.760000                  25  ...     0     0        0   \n",
       "4                  0.933333                  15  ...     0     1        0   \n",
       "\n",
       "   work  world  would  yes  you  you know  your  \n",
       "0     0      0      0    0    0         0     0  \n",
       "1     0      0      0    0    0         0     0  \n",
       "2     0      0      0    0    0         0     0  \n",
       "3     0      0      0    0    0         0     0  \n",
       "4     0      0      0    0    0         0     0  \n",
       "\n",
       "[5 rows x 336 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetar os índices de ambos os DataFrames\n",
    "df_test_reset = df_test.reset_index(drop=True)\n",
    "ngrams_df_reset = ngrams_df.reset_index(drop=True)\n",
    "\n",
    "# Agora concatenar\n",
    "df_combined = pd.concat([df_test_reset, ngrams_df_reset], axis=1)\n",
    "df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n",
    "df_combined.fillna(0)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e832d3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>with ai</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you know</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0     1.0                  0.818182                  22                    18   \n",
       "1     1.0                  0.952381                  21                    20   \n",
       "2     1.0                  0.914286                  35                    32   \n",
       "3     1.0                  0.760000                  25                    19   \n",
       "4     1.0                  0.933333                  15                    14   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 4.318182                      8.216942   \n",
       "1                 5.476190                     11.392290   \n",
       "2                 4.714286                      5.632653   \n",
       "3                 4.880000                      9.785600   \n",
       "4                 5.533333                     10.782222   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                0.181818                         0                         0   \n",
       "1                0.190476                         1                         0   \n",
       "2                0.228571                         1                         0   \n",
       "3                0.360000                         0                         0   \n",
       "4                0.200000                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  ...  will  with  with ai  work  world  \\\n",
       "0                            0.0  ...     0     0        0     0      0   \n",
       "1                            0.0  ...     0     1        0     0      0   \n",
       "2                            0.0  ...     0     0        0     0      0   \n",
       "3                            0.0  ...     0     0        0     0      0   \n",
       "4                            0.0  ...     0     1        0     0      0   \n",
       "\n",
       "   would  yes  you  you know  your  \n",
       "0      0    0    0         0     0  \n",
       "1      0    0    0         0     0  \n",
       "2      0    0    0         0     0  \n",
       "3      0    0    0         0     0  \n",
       "4      0    0    0         0     0  \n",
       "\n",
       "[5 rows x 329 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = df_combined.drop(columns=['topic', 'model', 'tweet_id', 'text', 'length', 'batch_timestamp', 'characters_remaining']) \n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d873fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>with ai</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you know</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0     1.0                  0.818182                  22                    18   \n",
       "1     1.0                  0.952381                  21                    20   \n",
       "2     1.0                  0.914286                  35                    32   \n",
       "3     1.0                  0.760000                  25                    19   \n",
       "4     1.0                  0.933333                  15                    14   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 4.318182                      8.216942   \n",
       "1                 5.476190                     11.392290   \n",
       "2                 4.714286                      5.632653   \n",
       "3                 4.880000                      9.785600   \n",
       "4                 5.533333                     10.782222   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                0.181818                         0                         0   \n",
       "1                0.190476                         1                         0   \n",
       "2                0.228571                         1                         0   \n",
       "3                0.360000                         0                         0   \n",
       "4                0.200000                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  ...  will  with  with ai  work  world  \\\n",
       "0                            0.0  ...     0     0        0     0      0   \n",
       "1                            0.0  ...     0     1        0     0      0   \n",
       "2                            0.0  ...     0     0        0     0      0   \n",
       "3                            0.0  ...     0     0        0     0      0   \n",
       "4                            0.0  ...     0     1        0     0      0   \n",
       "\n",
       "   would  yes  you  you know  your  \n",
       "0      0    0    0         0     0  \n",
       "1      0    0    0         0     0  \n",
       "2      0    0    0         0     0  \n",
       "3      0    0    0         0     0  \n",
       "4      0    0    0         0     0  \n",
       "\n",
       "[5 rows x 329 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['origin'] = df_combined['origin'].fillna(1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c495a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"/home/tammy.kojima/Authorship-attribution/df_pronto/df_geral_com_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21b355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>...</th>\n",
       "      <th>chatgpt_structured_output</th>\n",
       "      <th>chatgpt_overly_polite</th>\n",
       "      <th>chatgpt_disclaimer_density</th>\n",
       "      <th>chatgpt_assistant_patterns</th>\n",
       "      <th>mistral_self_ref</th>\n",
       "      <th>mistral_structured_density</th>\n",
       "      <th>mistral_technical_jargon</th>\n",
       "      <th>mistral_non_english_density</th>\n",
       "      <th>mistral_step_reasoning</th>\n",
       "      <th>mistral_low_ethical_disclaimers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3.689655</td>\n",
       "      <td>5.317479</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>4.352941</td>\n",
       "      <td>10.581315</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>3.925926</td>\n",
       "      <td>5.994513</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>46.834711</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>3.370370</td>\n",
       "      <td>3.640604</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 356 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin  lexical_type_token_ratio  lexical_word_count  \\\n",
       "0         1.0                  0.818182                  22   \n",
       "1         1.0                  0.952381                  21   \n",
       "2         1.0                  0.914286                  35   \n",
       "3         1.0                  0.760000                  25   \n",
       "4         1.0                  0.933333                  15   \n",
       "...       ...                       ...                 ...   \n",
       "19995     0.0                  0.758621                  29   \n",
       "19996     0.0                  1.000000                  17   \n",
       "19997     0.0                  0.925926                  27   \n",
       "19998     0.0                  0.818182                  22   \n",
       "19999     0.0                  0.962963                  27   \n",
       "\n",
       "       lexical_unique_words  lexical_avg_word_length  \\\n",
       "0                        18                 4.318182   \n",
       "1                        20                 5.476190   \n",
       "2                        32                 4.714286   \n",
       "3                        19                 4.880000   \n",
       "4                        14                 5.533333   \n",
       "...                     ...                      ...   \n",
       "19995                    22                 3.689655   \n",
       "19996                    17                 4.352941   \n",
       "19997                    25                 3.925926   \n",
       "19998                    18                 4.727273   \n",
       "19999                    26                 3.370370   \n",
       "\n",
       "       lexical_word_length_variance  lexical_stopword_ratio  \\\n",
       "0                          8.216942                0.181818   \n",
       "1                         11.392290                0.190476   \n",
       "2                          5.632653                0.228571   \n",
       "3                          9.785600                0.360000   \n",
       "4                         10.782222                0.200000   \n",
       "...                             ...                     ...   \n",
       "19995                      5.317479                0.344828   \n",
       "19996                     10.581315                0.352941   \n",
       "19997                      5.994513                0.185185   \n",
       "19998                     46.834711                0.181818   \n",
       "19999                      3.640604                0.259259   \n",
       "\n",
       "       stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                             0                         0   \n",
       "1                             1                         0   \n",
       "2                             1                         0   \n",
       "3                             0                         0   \n",
       "4                             0                         0   \n",
       "...                         ...                       ...   \n",
       "19995                         0                         0   \n",
       "19996                         0                         0   \n",
       "19997                         1                         0   \n",
       "19998                         1                         0   \n",
       "19999                         0                         0   \n",
       "\n",
       "       stylistic_exclamation_density  ...  chatgpt_structured_output  \\\n",
       "0                           0.000000  ...                        0.0   \n",
       "1                           0.000000  ...                        0.0   \n",
       "2                           0.000000  ...                        0.0   \n",
       "3                           0.000000  ...                        0.0   \n",
       "4                           0.000000  ...                        0.0   \n",
       "...                              ...  ...                        ...   \n",
       "19995                       0.103448  ...                        0.0   \n",
       "19996                       0.000000  ...                        0.0   \n",
       "19997                       0.000000  ...                        0.0   \n",
       "19998                       0.090909  ...                        0.0   \n",
       "19999                       0.074074  ...                        0.0   \n",
       "\n",
       "       chatgpt_overly_polite  chatgpt_disclaimer_density  \\\n",
       "0                        0.0                         0.0   \n",
       "1                        0.0                         0.0   \n",
       "2                        0.0                         0.0   \n",
       "3                        0.0                         0.0   \n",
       "4                        0.0                         0.0   \n",
       "...                      ...                         ...   \n",
       "19995                    0.0                         0.0   \n",
       "19996                    0.0                         0.0   \n",
       "19997                    0.0                         0.0   \n",
       "19998                    0.0                         0.0   \n",
       "19999                    0.0                         0.0   \n",
       "\n",
       "       chatgpt_assistant_patterns  mistral_self_ref  \\\n",
       "0                             0.0               0.0   \n",
       "1                             0.0               0.0   \n",
       "2                             0.0               0.0   \n",
       "3                             0.0               0.0   \n",
       "4                             0.0               0.0   \n",
       "...                           ...               ...   \n",
       "19995                         0.0               0.0   \n",
       "19996                         0.0               0.0   \n",
       "19997                         0.0               0.0   \n",
       "19998                         0.0               0.0   \n",
       "19999                         0.0               0.0   \n",
       "\n",
       "       mistral_structured_density  mistral_technical_jargon  \\\n",
       "0                             0.0                       0.0   \n",
       "1                             0.0                       0.0   \n",
       "2                             0.0                       0.0   \n",
       "3                             0.0                       0.0   \n",
       "4                             0.0                       0.0   \n",
       "...                           ...                       ...   \n",
       "19995                         0.0                       0.0   \n",
       "19996                         0.0                       0.0   \n",
       "19997                         0.0                       0.0   \n",
       "19998                         0.0                       0.0   \n",
       "19999                         0.0                       0.0   \n",
       "\n",
       "       mistral_non_english_density  mistral_step_reasoning  \\\n",
       "0                              0.0                     0.0   \n",
       "1                              0.0                     0.0   \n",
       "2                              0.0                     0.0   \n",
       "3                              0.0                     0.0   \n",
       "4                              0.0                     0.0   \n",
       "...                            ...                     ...   \n",
       "19995                          0.0                     0.0   \n",
       "19996                          0.0                     0.0   \n",
       "19997                          0.0                     0.0   \n",
       "19998                          0.0                     0.0   \n",
       "19999                          0.0                     0.0   \n",
       "\n",
       "       mistral_low_ethical_disclaimers  \n",
       "0                                  0.0  \n",
       "1                                  0.0  \n",
       "2                                  0.0  \n",
       "3                                  0.0  \n",
       "4                                  0.0  \n",
       "...                                ...  \n",
       "19995                              0.0  \n",
       "19996                              0.0  \n",
       "19997                              0.0  \n",
       "19998                              0.0  \n",
       "19999                              0.0  \n",
       "\n",
       "[20000 rows x 356 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_geral_com_features2.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78931d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>...</th>\n",
       "      <th>chatgpt_structured_output</th>\n",
       "      <th>chatgpt_overly_polite</th>\n",
       "      <th>chatgpt_disclaimer_density</th>\n",
       "      <th>chatgpt_assistant_patterns</th>\n",
       "      <th>mistral_self_ref</th>\n",
       "      <th>mistral_structured_density</th>\n",
       "      <th>mistral_technical_jargon</th>\n",
       "      <th>mistral_non_english_density</th>\n",
       "      <th>mistral_step_reasoning</th>\n",
       "      <th>mistral_low_ethical_disclaimers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4.318182</td>\n",
       "      <td>8.216942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5.476190</td>\n",
       "      <td>11.392290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.632653</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>9.785600</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>10.782222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 356 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0     1.0                  0.818182                  22                    18   \n",
       "1     1.0                  0.952381                  21                    20   \n",
       "2     1.0                  0.914286                  35                    32   \n",
       "3     1.0                  0.760000                  25                    19   \n",
       "4     1.0                  0.933333                  15                    14   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 4.318182                      8.216942   \n",
       "1                 5.476190                     11.392290   \n",
       "2                 4.714286                      5.632653   \n",
       "3                 4.880000                      9.785600   \n",
       "4                 5.533333                     10.782222   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                0.181818                         0                         0   \n",
       "1                0.190476                         1                         0   \n",
       "2                0.228571                         1                         0   \n",
       "3                0.360000                         0                         0   \n",
       "4                0.200000                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  ...  chatgpt_structured_output  \\\n",
       "0                            0.0  ...                        0.0   \n",
       "1                            0.0  ...                        0.0   \n",
       "2                            0.0  ...                        0.0   \n",
       "3                            0.0  ...                        0.0   \n",
       "4                            0.0  ...                        0.0   \n",
       "\n",
       "   chatgpt_overly_polite  chatgpt_disclaimer_density  \\\n",
       "0                    0.0                         0.0   \n",
       "1                    0.0                         0.0   \n",
       "2                    0.0                         0.0   \n",
       "3                    0.0                         0.0   \n",
       "4                    0.0                         0.0   \n",
       "\n",
       "   chatgpt_assistant_patterns  mistral_self_ref  mistral_structured_density  \\\n",
       "0                         0.0               0.0                         0.0   \n",
       "1                         0.0               0.0                         0.0   \n",
       "2                         0.0               0.0                         0.0   \n",
       "3                         0.0               0.0                         0.0   \n",
       "4                         0.0               0.0                         0.0   \n",
       "\n",
       "   mistral_technical_jargon  mistral_non_english_density  \\\n",
       "0                       0.0                          0.0   \n",
       "1                       0.0                          0.0   \n",
       "2                       0.0                          0.0   \n",
       "3                       0.0                          0.0   \n",
       "4                       0.0                          0.0   \n",
       "\n",
       "   mistral_step_reasoning  mistral_low_ethical_disclaimers  \n",
       "0                     0.0                              0.0  \n",
       "1                     0.0                              0.0  \n",
       "2                     0.0                              0.0  \n",
       "3                     0.0                              0.0  \n",
       "4                     0.0                              0.0  \n",
       "\n",
       "[5 rows x 356 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_geral_com_features.csv')\n",
    "X_new_features = fe.extract_features_batch(textos, processed_texts = processed_texts)\n",
    "df_features = pd.DataFrame(X_new_features)\n",
    "df_combined = pd.concat([df_combined, df_features], axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26435b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_geral_com_features2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meu_ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
