{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c259fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-30 13:20:33,741\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-30 13:20:35,764\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/tammy.kojima/Authorship-attribution/')\n",
    "import feature_extraction as fe\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273475dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_human.csv')\n",
    "df_human = df_human.iloc[:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c94c898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizando textos:   0%|                                                                                                        | 0/80000 [00:00<?, ?it/s][nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "Tokenizando textos: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 80000/80000 [00:04<00:00, 16512.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza todos os textos primeiro\n",
    "textos = df_human[\"text\"].fillna(\"\").tolist()\n",
    "processed_texts = [fe.preprocess_text(texto) for texto in tqdm(textos, desc=\"Tokenizando textos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0a1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estilísticas: 100%|████████████████████████████████████████████████████████████████████████████| 80000/80000 [00:01<00:00, 69220.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...  human    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...  human    NaN   \n",
       "2  or apparently it just magically worked this ti...  human    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...  human    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...  human    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  \n",
       "0                0.290323  \n",
       "1                0.150000  \n",
       "2                0.222222  \n",
       "3                0.250000  \n",
       "4                0.090909  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_features = [\n",
    "    fe.extract_lexical_features(textos[i], processed_texts[i])\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_lexical = pd.DataFrame(lexical_features)\n",
    "df_test = pd.concat([df_human, df_lexical], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53a7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estilísticas: 100%|███████████████████████████████████████████████████████████████████████████| 80000/80000 [00:00<00:00, 118478.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>stylistic_question_density</th>\n",
       "      <th>stylistic_ellipsis_count</th>\n",
       "      <th>stylistic_emoji_density</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...  human    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...  human    NaN   \n",
       "2  or apparently it just magically worked this ti...  human    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...  human    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...  human    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                0.290323                         0                         0   \n",
       "1                0.150000                         0                         0   \n",
       "2                0.222222                         0                         0   \n",
       "3                0.250000                         0                         0   \n",
       "4                0.090909                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  stylistic_question_density  \\\n",
       "0                       0.000000                    0.000000   \n",
       "1                       0.000000                    0.000000   \n",
       "2                       0.000000                    0.000000   \n",
       "3                       0.000000                    0.000000   \n",
       "4                       0.045455                    0.045455   \n",
       "\n",
       "   stylistic_ellipsis_count  stylistic_emoji_density  \\\n",
       "0                         0                      0.0   \n",
       "1                         0                      0.0   \n",
       "2                         0                      0.0   \n",
       "3                         0                      0.0   \n",
       "4                         0                      0.0   \n",
       "\n",
       "   stylistic_emoticon_density  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         0.0  \n",
       "4                         0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylistic_features = [\n",
    "    fe.extract_stylistic_features(textos[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_stylistic = pd.DataFrame(stylistic_features)\n",
    "df_test = pd.concat([df_test, df_stylistic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e4cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estruturais: 100%|████████████████████████████████████████████████████████████████████████████| 80000/80000 [00:00<00:00, 133504.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "      <th>structural_has_url</th>\n",
       "      <th>structural_has_mention</th>\n",
       "      <th>structural_has_hashtag</th>\n",
       "      <th>structural_is_retweet</th>\n",
       "      <th>structural_url_density</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...  human    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...  human    NaN   \n",
       "2  or apparently it just magically worked this ti...  human    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...  human    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...  human    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.290323                         0  ...   \n",
       "1                0.150000                         0  ...   \n",
       "2                0.222222                         0  ...   \n",
       "3                0.250000                         0  ...   \n",
       "4                0.090909                         0  ...   \n",
       "\n",
       "   stylistic_emoticon_density  structural_has_url  structural_has_mention  \\\n",
       "0                         0.0                   0                       1   \n",
       "1                         0.0                   0                       1   \n",
       "2                         0.0                   0                       0   \n",
       "3                         0.0                   0                       0   \n",
       "4                         0.0                   0                       0   \n",
       "\n",
       "   structural_has_hashtag  structural_is_retweet  structural_url_density  \\\n",
       "0                       0                      0                     0.0   \n",
       "1                       0                      0                     0.0   \n",
       "2                       0                      0                     0.0   \n",
       "3                       0                      0                     0.0   \n",
       "4                       0                      0                     0.0   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                    0.032258                         0.0   \n",
       "1                    0.050000                         0.0   \n",
       "2                    0.000000                         0.0   \n",
       "3                    0.000000                         0.0   \n",
       "4                    0.000000                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \n",
       "0                   0.0000                          0.0  \n",
       "1                   0.0000                          0.0  \n",
       "2                   0.0000                          0.0  \n",
       "3                   0.0625                          0.0  \n",
       "4                   0.0000                          0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_features = [\n",
    "    fe.extract_structural_features(textos[i], processed_texts[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estruturais\")\n",
    "]\n",
    "df_structural = pd.DataFrame(structural_features)\n",
    "df_test = pd.concat([df_test, df_structural], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033c2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features sintáticas: 100%|████████████████████████████████████████████████████████████████████████████████| 80000/80000 [04:55<00:00, 270.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>syntactic_subordinating_conj</th>\n",
       "      <th>syntactic_comma_ratio</th>\n",
       "      <th>syntactic_punct_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.116684</td>\n",
       "      <td>3.059355</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.029396</td>\n",
       "      <td>2.830940</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.212257</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.046739</td>\n",
       "      <td>2.523211</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.212427</td>\n",
       "      <td>2.965016</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...  human    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...  human    NaN   \n",
       "2  or apparently it just magically worked this ti...  human    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...  human    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...  human    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.290323                         0  ...   \n",
       "1                0.150000                         0  ...   \n",
       "2                0.222222                         0  ...   \n",
       "3                0.250000                         0  ...   \n",
       "4                0.090909                         0  ...   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                    0.032258                         0.0   \n",
       "1                    0.050000                         0.0   \n",
       "2                    0.000000                         0.0   \n",
       "3                    0.000000                         0.0   \n",
       "4                    0.000000                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \\\n",
       "0                   0.0000                          0.0   \n",
       "1                   0.0000                          0.0   \n",
       "2                   0.0000                          0.0   \n",
       "3                   0.0625                          0.0   \n",
       "4                   0.0000                          0.0   \n",
       "\n",
       "   syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "0                   2.116684                      3.059355   \n",
       "1                   2.029396                      2.830940   \n",
       "2                   2.212257                      2.833213   \n",
       "3                   2.046739                      2.523211   \n",
       "4                   2.212427                      2.965016   \n",
       "\n",
       "   syntactic_avg_sentence_length  syntactic_subordinating_conj  \\\n",
       "0                      30.000000                           0.0   \n",
       "1                       5.666667                           0.0   \n",
       "2                       8.500000                           0.0   \n",
       "3                       6.000000                           0.0   \n",
       "4                       9.000000                           0.0   \n",
       "\n",
       "   syntactic_comma_ratio  syntactic_punct_ratio  \n",
       "0               0.064516               0.064516  \n",
       "1               0.050000               0.200000  \n",
       "2               0.000000               0.055556  \n",
       "3               0.000000               0.250000  \n",
       "4               0.090909               0.227273  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features = [\n",
    "    fe.extract_syntactic_features(tokens)\n",
    "    for tokens in tqdm(processed_texts, desc=\"Extraindo features sintáticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(syntactic_features)\n",
    "df_test = pd.concat([df_test, df_syntactic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d101dc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2014</th>\n",
       "      <th>2019</th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>against</th>\n",
       "      <th>all</th>\n",
       "      <th>all the</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you are</th>\n",
       "      <th>you have</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2014  2019  about  after  again  against  all  all the  already  also  ...  \\\n",
       "0     0     0      0      0      0        0    0        0        0     0  ...   \n",
       "1     0     0      0      0      1        0    0        0        0     0  ...   \n",
       "2     0     0      1      0      0        0    0        0        0     0  ...   \n",
       "3     0     0      0      0      0        0    0        0        0     0  ...   \n",
       "4     0     0      1      0      0        0    0        0        0     1  ...   \n",
       "\n",
       "   world  would  yeah  year  years  yes  you  you are  you have  your  \n",
       "0      0      0     0     0      0    0    3        0         0     0  \n",
       "1      0      0     0     0      0    0    2        0         0     0  \n",
       "2      0      0     0     0      0    0    1        1         0     0  \n",
       "3      0      0     0     0      0    0    0        0         0     0  \n",
       "4      0      0     0     0      0    0    0        0         0     0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_df = fe.extract_ngrams_features(processed_texts, max_features=300)\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82286167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3453280/3577768971.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you are</th>\n",
       "      <th>you have</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...       0    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...       0    NaN   \n",
       "2  or apparently it just magically worked this ti...       0    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...       0    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...       0    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  world  would  yeah  \\\n",
       "0                0.290323                         0  ...      0      0     0   \n",
       "1                0.150000                         0  ...      0      0     0   \n",
       "2                0.222222                         0  ...      0      0     0   \n",
       "3                0.250000                         0  ...      0      0     0   \n",
       "4                0.090909                         0  ...      0      0     0   \n",
       "\n",
       "   year  years  yes  you  you are  you have  your  \n",
       "0     0      0    0    3        0         0     0  \n",
       "1     0      0    0    2        0         0     0  \n",
       "2     0      0    0    1        1         0     0  \n",
       "3     0      0    0    0        0         0     0  \n",
       "4     0      0    0    0        0         0     0  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetar os índices de ambos os DataFrames\n",
    "df_test_reset = df_test.reset_index(drop=True)\n",
    "ngrams_df_reset = ngrams_df.reset_index(drop=True)\n",
    "\n",
    "# Agora concatenar\n",
    "df_combined = pd.concat([df_test_reset, ngrams_df_reset], axis=1)\n",
    "df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n",
    "df_combined.fillna(0)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2cc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"/home/tammy.kojima/Authorship-attribution/df_pronto/df_human_com_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c1aa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>chatgpt_structured_output</th>\n",
       "      <th>chatgpt_overly_polite</th>\n",
       "      <th>chatgpt_disclaimer_density</th>\n",
       "      <th>chatgpt_assistant_patterns</th>\n",
       "      <th>mistral_self_ref</th>\n",
       "      <th>mistral_structured_density</th>\n",
       "      <th>mistral_technical_jargon</th>\n",
       "      <th>mistral_non_english_density</th>\n",
       "      <th>mistral_step_reasoning</th>\n",
       "      <th>mistral_low_ethical_disclaimers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hlooman Hans I'm an open book you can ask me ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>3.483871</td>\n",
       "      <td>2.701353</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DavidArchie Take care. Hope to see you again ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or apparently it just magically worked this ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * * this was almost perfect weekend with a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>6.152344</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RALLY INFRONT OF MC?! Yup, I'm also scared abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.231405</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 358 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  origin  model  \\\n",
       "0  @hlooman Hans I'm an open book you can ask me ...       0    NaN   \n",
       "1  @DavidArchie Take care. Hope to see you again ...       0    NaN   \n",
       "2  or apparently it just magically worked this ti...       0    NaN   \n",
       "3  * * * this was almost perfect weekend with a p...       0    NaN   \n",
       "4  RALLY INFRONT OF MC?! Yup, I'm also scared abo...       0    NaN   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.838710                  31                    26   \n",
       "1                  0.900000                  20                    18   \n",
       "2                  1.000000                  18                    18   \n",
       "3                  0.750000                  16                    12   \n",
       "4                  0.954545                  22                    21   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 3.483871                      2.701353   \n",
       "1                 4.000000                      6.700000   \n",
       "2                 4.833333                      6.805556   \n",
       "3                 3.812500                      6.152344   \n",
       "4                 3.363636                      3.231405   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.290323                         0  ...   \n",
       "1                0.150000                         0  ...   \n",
       "2                0.222222                         0  ...   \n",
       "3                0.250000                         0  ...   \n",
       "4                0.090909                         0  ...   \n",
       "\n",
       "   chatgpt_structured_output  chatgpt_overly_polite  \\\n",
       "0                     0.0000                    0.0   \n",
       "1                     0.0000                    0.0   \n",
       "2                     0.0000                    0.0   \n",
       "3                     0.1875                    0.0   \n",
       "4                     0.0000                    0.0   \n",
       "\n",
       "   chatgpt_disclaimer_density  chatgpt_assistant_patterns  mistral_self_ref  \\\n",
       "0                         0.0                         0.0               0.0   \n",
       "1                         0.0                         0.0               0.0   \n",
       "2                         0.0                         0.0               0.0   \n",
       "3                         0.0                         0.0               0.0   \n",
       "4                         0.0                         0.0               0.0   \n",
       "\n",
       "   mistral_structured_density  mistral_technical_jargon  \\\n",
       "0                      0.0000                       0.0   \n",
       "1                      0.0000                       0.0   \n",
       "2                      0.0000                       0.0   \n",
       "3                      0.1875                       0.0   \n",
       "4                      0.0000                       0.0   \n",
       "\n",
       "   mistral_non_english_density  mistral_step_reasoning  \\\n",
       "0                          0.0                    0.00   \n",
       "1                          0.0                    0.05   \n",
       "2                          0.0                    0.00   \n",
       "3                          0.0                    0.00   \n",
       "4                          0.0                    0.00   \n",
       "\n",
       "   mistral_low_ethical_disclaimers  \n",
       "0                              0.0  \n",
       "1                              0.0  \n",
       "2                              0.0  \n",
       "3                              0.0  \n",
       "4                              0.0  \n",
       "\n",
       "[5 rows x 358 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_human_com_features.csv')\n",
    "X_new_features = fe.extract_features_batch(textos, processed_texts = processed_texts)\n",
    "df_features = pd.DataFrame(X_new_features)\n",
    "df_combined = pd.concat([df_combined, df_features], axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568f82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_human_com_features2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features2.csv\")\n",
    "df_human = pd.read_csv(\"/home/tammy.kojima/Authorship-attribution/df_pronto/df_human_com_features2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d08a124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_random_uppercase</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>vote for</th>\n",
       "      <th>wait</th>\n",
       "      <th>watch</th>\n",
       "      <th>watching</th>\n",
       "      <th>wish</th>\n",
       "      <th>with modi</th>\n",
       "      <th>without</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you have</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202390</th>\n",
       "      <td>you comparison based the assumptions that modi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>5.354839</td>\n",
       "      <td>5.906348</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202391</th>\n",
       "      <td>choukidar means daythis modi days modi finishe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>5.307692</td>\n",
       "      <td>3.084813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202392</th>\n",
       "      <td>madam you have agree modi didnt had any corrup...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.860000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202393</th>\n",
       "      <td>where 15lakhs from swiss smart city promised m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>1.876543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202394</th>\n",
       "      <td>poor child and bua the other child was modi’ n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>1.785124</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202395 rows × 463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  origin  \\\n",
       "0       Our rules are the most robust, the president c...       1   \n",
       "1       ...associating outsiders with danger and extre...       1   \n",
       "2       ...alarms were raised in the initial stages of...       1   \n",
       "3       [The opposition] utilizes every tool to questi...       1   \n",
       "4       [The governor's] fixation with health protocol...       1   \n",
       "...                                                   ...     ...   \n",
       "202390  you comparison based the assumptions that modi...       0   \n",
       "202391  choukidar means daythis modi days modi finishe...       0   \n",
       "202392  madam you have agree modi didnt had any corrup...       0   \n",
       "202393  where 15lakhs from swiss smart city promised m...       0   \n",
       "202394  poor child and bua the other child was modi’ n...       0   \n",
       "\n",
       "        lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                       0.882353                  17                    15   \n",
       "1                       1.000000                  18                    18   \n",
       "2                       0.937500                  32                    30   \n",
       "3                       0.727273                  22                    16   \n",
       "4                       0.923077                  13                    12   \n",
       "...                          ...                 ...                   ...   \n",
       "202390                  0.967742                  31                    30   \n",
       "202391                  0.641026                  39                    25   \n",
       "202392                  0.900000                  20                    18   \n",
       "202393                  1.000000                   9                     9   \n",
       "202394                  0.909091                  11                    10   \n",
       "\n",
       "        lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      5.294118                      6.442907   \n",
       "1                      6.000000                      9.000000   \n",
       "2                      4.968750                      4.655273   \n",
       "3                      5.409091                      8.787190   \n",
       "4                      6.153846                      7.207101   \n",
       "...                         ...                           ...   \n",
       "202390                 5.354839                      5.906348   \n",
       "202391                 5.307692                      3.084813   \n",
       "202392                 5.200000                      5.860000   \n",
       "202393                 5.111111                      1.876543   \n",
       "202394                 3.818182                      1.785124   \n",
       "\n",
       "        lexical_stopword_ratio  stylistic_random_uppercase  \\\n",
       "0                     0.411765                         0.0   \n",
       "1                     0.277778                         0.0   \n",
       "2                     0.437500                         0.0   \n",
       "3                     0.500000                         0.0   \n",
       "4                     0.384615                         0.0   \n",
       "...                        ...                         ...   \n",
       "202390                0.129032                         NaN   \n",
       "202391                0.000000                         NaN   \n",
       "202392                0.150000                         NaN   \n",
       "202393                0.000000                         NaN   \n",
       "202394                0.272727                         NaN   \n",
       "\n",
       "        stylistic_repeated_chars  ...  vote for  wait  watch  watching  wish  \\\n",
       "0                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "1                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "2                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "3                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "4                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "...                          ...  ...       ...   ...    ...       ...   ...   \n",
       "202390                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202391                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202392                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202393                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202394                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "\n",
       "        with modi  without  yeah  yes  you have  \n",
       "0             NaN      NaN   NaN  NaN       NaN  \n",
       "1             NaN      NaN   NaN  NaN       NaN  \n",
       "2             NaN      NaN   NaN  NaN       NaN  \n",
       "3             NaN      NaN   NaN  NaN       NaN  \n",
       "4             NaN      NaN   NaN  NaN       NaN  \n",
       "...           ...      ...   ...  ...       ...  \n",
       "202390        0.0      0.0   0.0  0.0       0.0  \n",
       "202391        0.0      0.0   0.0  0.0       0.0  \n",
       "202392        0.0      0.0   0.0  0.0       1.0  \n",
       "202393        0.0      0.0   0.0  0.0       0.0  \n",
       "202394        0.0      0.0   0.0  0.0       0.0  \n",
       "\n",
       "[202395 rows x 463 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo = pd.concat([df, df_human], ignore_index=True)\n",
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4df2752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_random_uppercase</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>vote for</th>\n",
       "      <th>wait</th>\n",
       "      <th>watch</th>\n",
       "      <th>watching</th>\n",
       "      <th>wish</th>\n",
       "      <th>with modi</th>\n",
       "      <th>without</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you have</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202390</th>\n",
       "      <td>you comparison based the assumptions that modi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>5.354839</td>\n",
       "      <td>5.906348</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202391</th>\n",
       "      <td>choukidar means daythis modi days modi finishe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>5.307692</td>\n",
       "      <td>3.084813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202392</th>\n",
       "      <td>madam you have agree modi didnt had any corrup...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.860000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202393</th>\n",
       "      <td>where 15lakhs from swiss smart city promised m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>1.876543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202394</th>\n",
       "      <td>poor child and bua the other child was modi’ n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>1.785124</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202395 rows × 463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  origin  \\\n",
       "0       Our rules are the most robust, the president c...       1   \n",
       "1       ...associating outsiders with danger and extre...       1   \n",
       "2       ...alarms were raised in the initial stages of...       1   \n",
       "3       [The opposition] utilizes every tool to questi...       1   \n",
       "4       [The governor's] fixation with health protocol...       1   \n",
       "...                                                   ...     ...   \n",
       "202390  you comparison based the assumptions that modi...       0   \n",
       "202391  choukidar means daythis modi days modi finishe...       0   \n",
       "202392  madam you have agree modi didnt had any corrup...       0   \n",
       "202393  where 15lakhs from swiss smart city promised m...       0   \n",
       "202394  poor child and bua the other child was modi’ n...       0   \n",
       "\n",
       "        lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                       0.882353                  17                    15   \n",
       "1                       1.000000                  18                    18   \n",
       "2                       0.937500                  32                    30   \n",
       "3                       0.727273                  22                    16   \n",
       "4                       0.923077                  13                    12   \n",
       "...                          ...                 ...                   ...   \n",
       "202390                  0.967742                  31                    30   \n",
       "202391                  0.641026                  39                    25   \n",
       "202392                  0.900000                  20                    18   \n",
       "202393                  1.000000                   9                     9   \n",
       "202394                  0.909091                  11                    10   \n",
       "\n",
       "        lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      5.294118                      6.442907   \n",
       "1                      6.000000                      9.000000   \n",
       "2                      4.968750                      4.655273   \n",
       "3                      5.409091                      8.787190   \n",
       "4                      6.153846                      7.207101   \n",
       "...                         ...                           ...   \n",
       "202390                 5.354839                      5.906348   \n",
       "202391                 5.307692                      3.084813   \n",
       "202392                 5.200000                      5.860000   \n",
       "202393                 5.111111                      1.876543   \n",
       "202394                 3.818182                      1.785124   \n",
       "\n",
       "        lexical_stopword_ratio  stylistic_random_uppercase  \\\n",
       "0                     0.411765                         0.0   \n",
       "1                     0.277778                         0.0   \n",
       "2                     0.437500                         0.0   \n",
       "3                     0.500000                         0.0   \n",
       "4                     0.384615                         0.0   \n",
       "...                        ...                         ...   \n",
       "202390                0.129032                         NaN   \n",
       "202391                0.000000                         NaN   \n",
       "202392                0.150000                         NaN   \n",
       "202393                0.000000                         NaN   \n",
       "202394                0.272727                         NaN   \n",
       "\n",
       "        stylistic_repeated_chars  ...  vote for  wait  watch  watching  wish  \\\n",
       "0                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "1                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "2                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "3                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "4                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "...                          ...  ...       ...   ...    ...       ...   ...   \n",
       "202390                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202391                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202392                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202393                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "202394                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "\n",
       "        with modi  without  yeah  yes  you have  \n",
       "0             NaN      NaN   NaN  NaN       NaN  \n",
       "1             NaN      NaN   NaN  NaN       NaN  \n",
       "2             NaN      NaN   NaN  NaN       NaN  \n",
       "3             NaN      NaN   NaN  NaN       NaN  \n",
       "4             NaN      NaN   NaN  NaN       NaN  \n",
       "...           ...      ...   ...  ...       ...  \n",
       "202390        0.0      0.0   0.0  0.0       0.0  \n",
       "202391        0.0      0.0   0.0  0.0       0.0  \n",
       "202392        0.0      0.0   0.0  0.0       1.0  \n",
       "202393        0.0      0.0   0.0  0.0       0.0  \n",
       "202394        0.0      0.0   0.0  0.0       0.0  \n",
       "\n",
       "[202395 rows x 463 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo = df_novo.drop_duplicates()\n",
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484f993a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "origin\n",
       "1    100000\n",
       "0    100000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo['origin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da694e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novo.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f5b5e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Colunas a serem removidas: 27\n",
      "Exemplos: ['ai_perplexity_score', 'ai_repeated_ngrams', 'ai_topic_shifts', 'ai_safety_disclaimers', 'ai_hedging_language', 'complexity_subordinating_ratio', 'complexity_long_sentences', 'complexity_rare_words', 'complexity_syntactic_depth', 'temporal_recent_ratio']\n",
      "✅ Shape após remoção: (200000, 436)\n"
     ]
    }
   ],
   "source": [
    "# Lista de prefixos para remover\n",
    "prefixos = ['ai_', 'llm_', 'mistral_', 'chatgpt_', 'temporal_', 'complexity_']\n",
    "\n",
    "# Encontrar colunas que começam com esses prefixos\n",
    "colunas_para_remover = [col for col in df.columns \n",
    "                       if any(col.startswith(prefixo) for prefixo in prefixos)]\n",
    "\n",
    "print(f\"📊 Colunas a serem removidas: {len(colunas_para_remover)}\")\n",
    "print(\"Exemplos:\", colunas_para_remover[:10])  # Mostra as primeiras 10\n",
    "\n",
    "# Remover as colunas\n",
    "df_limpo = df_novo.drop(colunas_para_remover, axis=1)\n",
    "print(f\"✅ Shape após remoção: {df_limpo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0621816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_random_uppercase</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>vote for</th>\n",
       "      <th>wait</th>\n",
       "      <th>watch</th>\n",
       "      <th>watching</th>\n",
       "      <th>wish</th>\n",
       "      <th>with modi</th>\n",
       "      <th>without</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you have</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>very happy best candidate for bangalore south ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>4.264463</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>you made country islam you are proud lets beli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>4.317073</td>\n",
       "      <td>2.070196</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>kannauj and allahabad will difficult for bjp t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>5.760000</td>\n",
       "      <td>8.502400</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>truth toh lakh aney hain aur 6000 bas choose k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>7.173333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>the election the whole country reelect modi wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>3.076389</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  origin  \\\n",
       "0       Our rules are the most robust, the president c...       1   \n",
       "1       ...associating outsiders with danger and extre...       1   \n",
       "2       ...alarms were raised in the initial stages of...       1   \n",
       "3       [The opposition] utilizes every tool to questi...       1   \n",
       "4       [The governor's] fixation with health protocol...       1   \n",
       "...                                                   ...     ...   \n",
       "199995  very happy best candidate for bangalore south ...       0   \n",
       "199996  you made country islam you are proud lets beli...       0   \n",
       "199997  kannauj and allahabad will difficult for bjp t...       0   \n",
       "199998  truth toh lakh aney hain aur 6000 bas choose k...       0   \n",
       "199999  the election the whole country reelect modi wi...       0   \n",
       "\n",
       "        lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                       0.882353                  17                    15   \n",
       "1                       1.000000                  18                    18   \n",
       "2                       0.937500                  32                    30   \n",
       "3                       0.727273                  22                    16   \n",
       "4                       0.923077                  13                    12   \n",
       "...                          ...                 ...                   ...   \n",
       "199995                  0.909091                  11                    10   \n",
       "199996                  0.731707                  41                    30   \n",
       "199997                  0.840000                  25                    21   \n",
       "199998                  0.933333                  15                    14   \n",
       "199999                  0.916667                  12                    11   \n",
       "\n",
       "        lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      5.294118                      6.442907   \n",
       "1                      6.000000                      9.000000   \n",
       "2                      4.968750                      4.655273   \n",
       "3                      5.409091                      8.787190   \n",
       "4                      6.153846                      7.207101   \n",
       "...                         ...                           ...   \n",
       "199995                 4.909091                      4.264463   \n",
       "199996                 4.317073                      2.070196   \n",
       "199997                 5.760000                      8.502400   \n",
       "199998                 5.600000                      7.173333   \n",
       "199999                 5.583333                      3.076389   \n",
       "\n",
       "        lexical_stopword_ratio  stylistic_random_uppercase  \\\n",
       "0                     0.411765                         0.0   \n",
       "1                     0.277778                         0.0   \n",
       "2                     0.437500                         0.0   \n",
       "3                     0.500000                         0.0   \n",
       "4                     0.384615                         0.0   \n",
       "...                        ...                         ...   \n",
       "199995                0.181818                         NaN   \n",
       "199996                0.195122                         NaN   \n",
       "199997                0.160000                         NaN   \n",
       "199998                0.000000                         NaN   \n",
       "199999                0.250000                         NaN   \n",
       "\n",
       "        stylistic_repeated_chars  ...  vote for  wait  watch  watching  wish  \\\n",
       "0                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "1                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "2                              1  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "3                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "4                              0  ...       NaN   NaN    NaN       NaN   NaN   \n",
       "...                          ...  ...       ...   ...    ...       ...   ...   \n",
       "199995                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "199996                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "199997                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "199998                         1  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "199999                         0  ...       0.0   0.0    0.0       0.0   0.0   \n",
       "\n",
       "        with modi  without  yeah  yes  you have  \n",
       "0             NaN      NaN   NaN  NaN       NaN  \n",
       "1             NaN      NaN   NaN  NaN       NaN  \n",
       "2             NaN      NaN   NaN  NaN       NaN  \n",
       "3             NaN      NaN   NaN  NaN       NaN  \n",
       "4             NaN      NaN   NaN  NaN       NaN  \n",
       "...           ...      ...   ...  ...       ...  \n",
       "199995        0.0      0.0   0.0  0.0       0.0  \n",
       "199996        0.0      0.0   0.0  0.0       0.0  \n",
       "199997        0.0      0.0   0.0  0.0       0.0  \n",
       "199998        0.0      0.0   0.0  0.0       0.0  \n",
       "199999        0.0      0.0   0.0  0.0       0.0  \n",
       "\n",
       "[200000 rows x 436 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741e39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meu_ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
