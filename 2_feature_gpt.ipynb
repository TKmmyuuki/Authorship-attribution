{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d7d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-26 14:08:36,053\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-26 14:08:36,608\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/tammy.kojima/Authorship-attribution/')\n",
    "import feature_extraction as fe\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e081c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>modi took revenge you bcos you are advani team...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>really doubt any organisation allowed function...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>⃣ already possession this capabilities but thi...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>although nasa doesnt have space suits for wome...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>modi one side and the other side its like comp...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text origin     model\n",
       "0       Our rules are the most robust, the president c...     AI  chat gpt\n",
       "1       ...associating outsiders with danger and extre...     AI  chat gpt\n",
       "2       ...alarms were raised in the initial stages of...     AI  chat gpt\n",
       "3       [The opposition] utilizes every tool to questi...     AI  chat gpt\n",
       "4       [The governor's] fixation with health protocol...     AI  chat gpt\n",
       "...                                                   ...    ...       ...\n",
       "199995  modi took revenge you bcos you are advani team...  human       NaN\n",
       "199996  really doubt any organisation allowed function...  human       NaN\n",
       "199997  ⃣ already possession this capabilities but thi...  human       NaN\n",
       "199998  although nasa doesnt have space suits for wome...  human       NaN\n",
       "199999  modi one side and the other side its like comp...  human       NaN\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt.csv')\n",
    "df_human = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_human.csv')\n",
    "df_human = df_human.iloc[:100000]\n",
    "df_test = pd.concat([df_gpt, df_human], ignore_index=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0ab0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza todos os textos primeiro\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()\n",
    "#processed_texts = [fe.preprocess_text(texto) for texto in tqdm(textos, desc=\"Tokenizando textos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e020747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prossessed_texts = pd.DataFrame({'text': processed_texts})\n",
    "#df_prossessed_texts.to_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_tokens_gpt.csv', index=False)\n",
    "df_prossessed_texts = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_tokens_gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7eb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = df_prossessed_texts[\"processed_text\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385b886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estilísticas: 100%|██████████████████████████████████████████████████████████████████████████| 200000/200000 [00:06<00:00, 31720.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.151899                 158                    24   \n",
       "1                  0.150000                 180                    27   \n",
       "2                  0.094077                 287                    27   \n",
       "3                  0.130435                 207                    27   \n",
       "4                  0.212121                 132                    28   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      1.0                           0.0   \n",
       "1                      1.0                           0.0   \n",
       "2                      1.0                           0.0   \n",
       "3                      1.0                           0.0   \n",
       "4                      1.0                           0.0   \n",
       "\n",
       "   lexical_stopword_ratio  \n",
       "0                0.063291  \n",
       "1                0.105556  \n",
       "2                0.097561  \n",
       "3                0.053140  \n",
       "4                0.083333  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_features = [\n",
    "    fe.extract_lexical_features(textos[i], processed_texts[i])\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_lexical = pd.DataFrame(lexical_features)\n",
    "df_test = pd.concat([df_test, df_lexical], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a67e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estilísticas: 100%|██████████████████████████████████████████████████████████████████████████| 200000/200000 [00:02<00:00, 96871.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>stylistic_question_density</th>\n",
       "      <th>stylistic_ellipsis_count</th>\n",
       "      <th>stylistic_emoji_density</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.151899                 158                    24   \n",
       "1                  0.150000                 180                    27   \n",
       "2                  0.094077                 287                    27   \n",
       "3                  0.130435                 207                    27   \n",
       "4                  0.212121                 132                    28   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      1.0                           0.0   \n",
       "1                      1.0                           0.0   \n",
       "2                      1.0                           0.0   \n",
       "3                      1.0                           0.0   \n",
       "4                      1.0                           0.0   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                0.063291                         0                         0   \n",
       "1                0.105556                         1                         0   \n",
       "2                0.097561                         1                         0   \n",
       "3                0.053140                         0                         0   \n",
       "4                0.083333                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  stylistic_question_density  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   stylistic_ellipsis_count  stylistic_emoji_density  \\\n",
       "0                         0                      0.0   \n",
       "1                         1                      0.0   \n",
       "2                         1                      0.0   \n",
       "3                         0                      0.0   \n",
       "4                         0                      0.0   \n",
       "\n",
       "   stylistic_emoticon_density  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         0.0  \n",
       "4                         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylistic_features = [\n",
    "    fe.extract_stylistic_features(textos[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_stylistic = pd.DataFrame(stylistic_features)\n",
    "df_test = pd.concat([df_test, df_stylistic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccde68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features estruturais: 100%|███████████████████████████████████████████████████████████████████████████| 200000/200000 [00:02<00:00, 67618.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "      <th>structural_has_url</th>\n",
       "      <th>structural_has_mention</th>\n",
       "      <th>structural_has_hashtag</th>\n",
       "      <th>structural_is_retweet</th>\n",
       "      <th>structural_url_density</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.151899                 158                    24   \n",
       "1                  0.150000                 180                    27   \n",
       "2                  0.094077                 287                    27   \n",
       "3                  0.130435                 207                    27   \n",
       "4                  0.212121                 132                    28   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      1.0                           0.0   \n",
       "1                      1.0                           0.0   \n",
       "2                      1.0                           0.0   \n",
       "3                      1.0                           0.0   \n",
       "4                      1.0                           0.0   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.063291                         0  ...   \n",
       "1                0.105556                         1  ...   \n",
       "2                0.097561                         1  ...   \n",
       "3                0.053140                         0  ...   \n",
       "4                0.083333                         0  ...   \n",
       "\n",
       "   stylistic_emoticon_density  structural_has_url  structural_has_mention  \\\n",
       "0                         0.0                   0                       0   \n",
       "1                         0.0                   0                       0   \n",
       "2                         0.0                   0                       0   \n",
       "3                         0.0                   0                       0   \n",
       "4                         0.0                   0                       0   \n",
       "\n",
       "   structural_has_hashtag  structural_is_retweet  structural_url_density  \\\n",
       "0                       0                      0                     0.0   \n",
       "1                       0                      0                     0.0   \n",
       "2                       0                      0                     0.0   \n",
       "3                       0                      0                     0.0   \n",
       "4                       0                      0                     0.0   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \n",
       "0                      0.0                          0.0  \n",
       "1                      0.0                          0.0  \n",
       "2                      0.0                          0.0  \n",
       "3                      0.0                          0.0  \n",
       "4                      0.0                          0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_features = [\n",
    "    fe.extract_structural_features(textos[i], processed_texts[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estruturais\")\n",
    "]\n",
    "df_structural = pd.DataFrame(structural_features)\n",
    "df_test = pd.concat([df_test, df_structural], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22084253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo features sintáticas: 100%|███████████████████████████████████████████████████████████████████████████████| 200000/200000 [52:34<00:00, 63.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>syntactic_subordinating_conj</th>\n",
       "      <th>syntactic_comma_ratio</th>\n",
       "      <th>syntactic_punct_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.684260</td>\n",
       "      <td>2.760874</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.329114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.877120</td>\n",
       "      <td>3.137987</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.863008</td>\n",
       "      <td>3.154021</td>\n",
       "      <td>13.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108014</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.602931</td>\n",
       "      <td>2.685808</td>\n",
       "      <td>10.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.323671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.808484</td>\n",
       "      <td>3.074954</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.310606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.151899                 158                    24   \n",
       "1                  0.150000                 180                    27   \n",
       "2                  0.094077                 287                    27   \n",
       "3                  0.130435                 207                    27   \n",
       "4                  0.212121                 132                    28   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      1.0                           0.0   \n",
       "1                      1.0                           0.0   \n",
       "2                      1.0                           0.0   \n",
       "3                      1.0                           0.0   \n",
       "4                      1.0                           0.0   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.063291                         0  ...   \n",
       "1                0.105556                         1  ...   \n",
       "2                0.097561                         1  ...   \n",
       "3                0.053140                         0  ...   \n",
       "4                0.083333                         0  ...   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \\\n",
       "0                      0.0                          0.0   \n",
       "1                      0.0                          0.0   \n",
       "2                      0.0                          0.0   \n",
       "3                      0.0                          0.0   \n",
       "4                      0.0                          0.0   \n",
       "\n",
       "   syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "0                   1.684260                      2.760874   \n",
       "1                   1.877120                      3.137987   \n",
       "2                   1.863008                      3.154021   \n",
       "3                   1.602931                      2.685808   \n",
       "4                   1.808484                      3.074954   \n",
       "\n",
       "   syntactic_avg_sentence_length  syntactic_subordinating_conj  \\\n",
       "0                      10.000000                           0.0   \n",
       "1                      12.000000                           0.0   \n",
       "2                      13.166667                           0.0   \n",
       "3                      10.818182                           0.0   \n",
       "4                      15.800000                           0.0   \n",
       "\n",
       "   syntactic_comma_ratio  syntactic_punct_ratio  \n",
       "0               0.101266               0.329114  \n",
       "1               0.094444               0.305556  \n",
       "2               0.108014               0.341463  \n",
       "3               0.101449               0.323671  \n",
       "4               0.090909               0.310606  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features = [\n",
    "    fe.extract_syntactic_features(tokens)\n",
    "    for tokens in tqdm(processed_texts, desc=\"Extraindo features sintáticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(syntactic_features)\n",
    "df_test = pd.concat([df_test, df_syntactic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc29965",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ngrams_df \u001b[38;5;241m=\u001b[39m \u001b[43mfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_ngrams_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ngrams_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Authorship-attribution/feature_extraction.py:191\u001b[0m, in \u001b[0;36mextract_ngrams_features\u001b[0;34m(processed_texts, max_features)\u001b[0m\n\u001b[1;32m    188\u001b[0m text_strings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m processed_texts]\n\u001b[1;32m    190\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), max_features\u001b[38;5;241m=\u001b[39mmax_features)\n\u001b[0;32m--> 191\u001b[0m X_ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_strings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(X_ngrams, columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             )\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m         )\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "ngrams_df = fe.extract_ngrams_features(processed_texts, max_features=300)\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef86219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar os índices de ambos os DataFrames\n",
    "df_test_reset = df_test.reset_index(drop=True)\n",
    "ngrams_df_reset = ngrams_df.reset_index(drop=True)\n",
    "df_combined = pd.concat([df_test_reset, ngrams_df_reset], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda10ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3447455/2107212330.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0}).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>syntactic_subordinating_conj</th>\n",
       "      <th>syntactic_comma_ratio</th>\n",
       "      <th>syntactic_punct_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.684260</td>\n",
       "      <td>2.760874</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.329114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.877120</td>\n",
       "      <td>3.137987</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.863008</td>\n",
       "      <td>3.154021</td>\n",
       "      <td>13.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108014</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.602931</td>\n",
       "      <td>2.685808</td>\n",
       "      <td>10.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.323671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.808484</td>\n",
       "      <td>3.074954</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.310606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  origin     model  \\\n",
       "0  Our rules are the most robust, the president c...       1  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...       1  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...       1  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...       1  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...       1  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.151899                 158                    24   \n",
       "1                  0.150000                 180                    27   \n",
       "2                  0.094077                 287                    27   \n",
       "3                  0.130435                 207                    27   \n",
       "4                  0.212121                 132                    28   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                      1.0                           0.0   \n",
       "1                      1.0                           0.0   \n",
       "2                      1.0                           0.0   \n",
       "3                      1.0                           0.0   \n",
       "4                      1.0                           0.0   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                0.063291                         0  ...   \n",
       "1                0.105556                         1  ...   \n",
       "2                0.097561                         1  ...   \n",
       "3                0.053140                         0  ...   \n",
       "4                0.083333                         0  ...   \n",
       "\n",
       "   structural_mention_density  structural_hashtag_density  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   structural_extra_spaces  structural_temporal_markers  \\\n",
       "0                      0.0                          0.0   \n",
       "1                      0.0                          0.0   \n",
       "2                      0.0                          0.0   \n",
       "3                      0.0                          0.0   \n",
       "4                      0.0                          0.0   \n",
       "\n",
       "   syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "0                   1.684260                      2.760874   \n",
       "1                   1.877120                      3.137987   \n",
       "2                   1.863008                      3.154021   \n",
       "3                   1.602931                      2.685808   \n",
       "4                   1.808484                      3.074954   \n",
       "\n",
       "   syntactic_avg_sentence_length  syntactic_subordinating_conj  \\\n",
       "0                      10.000000                           0.0   \n",
       "1                      12.000000                           0.0   \n",
       "2                      13.166667                           0.0   \n",
       "3                      10.818182                           0.0   \n",
       "4                      15.800000                           0.0   \n",
       "\n",
       "   syntactic_comma_ratio  syntactic_punct_ratio  \n",
       "0               0.101266               0.329114  \n",
       "1               0.094444               0.305556  \n",
       "2               0.108014               0.341463  \n",
       "3               0.101449               0.323671  \n",
       "4               0.090909               0.310606  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora concatenar\n",
    "df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0}).astype(int)\n",
    "df_combined.fillna(0)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d9a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3447455/2488752161.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_combined = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>syntactic_subordinating_conj</th>\n",
       "      <th>syntactic_comma_ratio</th>\n",
       "      <th>syntactic_punct_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.684260</td>\n",
       "      <td>2.760874</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.329114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.877120</td>\n",
       "      <td>3.137987</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>287</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.863008</td>\n",
       "      <td>3.154021</td>\n",
       "      <td>13.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108014</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.602931</td>\n",
       "      <td>2.685808</td>\n",
       "      <td>10.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.323671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.808484</td>\n",
       "      <td>3.074954</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.310606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>modi took revenge you bcos you are advani team...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>268</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108209</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.788780</td>\n",
       "      <td>3.144630</td>\n",
       "      <td>16.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108209</td>\n",
       "      <td>0.339552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>really doubt any organisation allowed function...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>135</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.637948</td>\n",
       "      <td>2.735214</td>\n",
       "      <td>10.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>⃣ already possession this capabilities but thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>303</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102310</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.891227</td>\n",
       "      <td>3.257440</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102310</td>\n",
       "      <td>0.320132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>although nasa doesnt have space suits for wome...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197080</td>\n",
       "      <td>137</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094891</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.850921</td>\n",
       "      <td>3.105339</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.379562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>modi one side and the other side its like comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.196850</td>\n",
       "      <td>127</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.804616</td>\n",
       "      <td>2.994964</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>0.362205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  origin     model  \\\n",
       "0       Our rules are the most robust, the president c...       1  chat gpt   \n",
       "1       ...associating outsiders with danger and extre...       1  chat gpt   \n",
       "2       ...alarms were raised in the initial stages of...       1  chat gpt   \n",
       "3       [The opposition] utilizes every tool to questi...       1  chat gpt   \n",
       "4       [The governor's] fixation with health protocol...       1  chat gpt   \n",
       "...                                                   ...     ...       ...   \n",
       "199995  modi took revenge you bcos you are advani team...       0       NaN   \n",
       "199996  really doubt any organisation allowed function...       0       NaN   \n",
       "199997  ⃣ already possession this capabilities but thi...       0       NaN   \n",
       "199998  although nasa doesnt have space suits for wome...       0       NaN   \n",
       "199999  modi one side and the other side its like comp...       0       NaN   \n",
       "\n",
       "        lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                       0.151899                 158                    24   \n",
       "1                       0.150000                 180                    27   \n",
       "2                       0.094077                 287                    27   \n",
       "3                       0.130435                 207                    27   \n",
       "4                       0.212121                 132                    28   \n",
       "...                          ...                 ...                   ...   \n",
       "199995                  0.100746                 268                    27   \n",
       "199996                  0.177778                 135                    24   \n",
       "199997                  0.099010                 303                    30   \n",
       "199998                  0.197080                 137                    27   \n",
       "199999                  0.196850                 127                    25   \n",
       "\n",
       "        lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                           1.0                           0.0   \n",
       "1                           1.0                           0.0   \n",
       "2                           1.0                           0.0   \n",
       "3                           1.0                           0.0   \n",
       "4                           1.0                           0.0   \n",
       "...                         ...                           ...   \n",
       "199995                      1.0                           0.0   \n",
       "199996                      1.0                           0.0   \n",
       "199997                      1.0                           0.0   \n",
       "199998                      1.0                           0.0   \n",
       "199999                      1.0                           0.0   \n",
       "\n",
       "        lexical_stopword_ratio  stylistic_repeated_chars  ...  \\\n",
       "0                     0.063291                         0  ...   \n",
       "1                     0.105556                         1  ...   \n",
       "2                     0.097561                         1  ...   \n",
       "3                     0.053140                         0  ...   \n",
       "4                     0.083333                         0  ...   \n",
       "...                        ...                       ...  ...   \n",
       "199995                0.108209                         0  ...   \n",
       "199996                0.111111                         0  ...   \n",
       "199997                0.102310                         0  ...   \n",
       "199998                0.094891                         0  ...   \n",
       "199999                0.110236                         0  ...   \n",
       "\n",
       "        structural_mention_density  structural_hashtag_density  \\\n",
       "0                              0.0                         0.0   \n",
       "1                              0.0                         0.0   \n",
       "2                              0.0                         0.0   \n",
       "3                              0.0                         0.0   \n",
       "4                              0.0                         0.0   \n",
       "...                            ...                         ...   \n",
       "199995                         0.0                         0.0   \n",
       "199996                         0.0                         0.0   \n",
       "199997                         0.0                         0.0   \n",
       "199998                         0.0                         0.0   \n",
       "199999                         0.0                         0.0   \n",
       "\n",
       "        structural_extra_spaces  structural_temporal_markers  \\\n",
       "0                      0.000000                          0.0   \n",
       "1                      0.000000                          0.0   \n",
       "2                      0.000000                          0.0   \n",
       "3                      0.000000                          0.0   \n",
       "4                      0.000000                          0.0   \n",
       "...                         ...                          ...   \n",
       "199995                 0.000000                          0.0   \n",
       "199996                 0.000000                          0.0   \n",
       "199997                 0.000000                          0.0   \n",
       "199998                 0.007299                          0.0   \n",
       "199999                 0.000000                          0.0   \n",
       "\n",
       "        syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "0                        1.684260                      2.760874   \n",
       "1                        1.877120                      3.137987   \n",
       "2                        1.863008                      3.154021   \n",
       "3                        1.602931                      2.685808   \n",
       "4                        1.808484                      3.074954   \n",
       "...                           ...                           ...   \n",
       "199995                   1.788780                      3.144630   \n",
       "199996                   1.637948                      2.735214   \n",
       "199997                   1.891227                      3.257440   \n",
       "199998                   1.850921                      3.105339   \n",
       "199999                   1.804616                      2.994964   \n",
       "\n",
       "        syntactic_avg_sentence_length  syntactic_subordinating_conj  \\\n",
       "0                           10.000000                           0.0   \n",
       "1                           12.000000                           0.0   \n",
       "2                           13.166667                           0.0   \n",
       "3                           10.818182                           0.0   \n",
       "4                           15.800000                           0.0   \n",
       "...                               ...                           ...   \n",
       "199995                      16.444444                           0.0   \n",
       "199996                      10.375000                           0.0   \n",
       "199997                       8.750000                           0.0   \n",
       "199998                      11.500000                           0.0   \n",
       "199999                       8.375000                           0.0   \n",
       "\n",
       "        syntactic_comma_ratio  syntactic_punct_ratio  \n",
       "0                    0.101266               0.329114  \n",
       "1                    0.094444               0.305556  \n",
       "2                    0.108014               0.341463  \n",
       "3                    0.101449               0.323671  \n",
       "4                    0.090909               0.310606  \n",
       "...                       ...                    ...  \n",
       "199995               0.108209               0.339552  \n",
       "199996               0.088889               0.296296  \n",
       "199997               0.102310               0.320132  \n",
       "199998               0.116788               0.379562  \n",
       "199999               0.110236               0.362205  \n",
       "\n",
       "[200000 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_combined.to_csv(\"/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features.csv\", index=False)\n",
    "df_combined = pd.read_csv('/home/tammy.kojima/Authorship-attribution/df_pronto/df_gpt_com_features.csv')\n",
    "df_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
