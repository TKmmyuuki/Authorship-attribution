{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc316a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tokenizer' from 'tweetnlp' (/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfe\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/Authorship-attribution/feature_extraction.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtweetnlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, NER\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tokenizer' from 'tweetnlp' (/home/tammy.kojima/miniconda3/envs/meu_ambiente/lib/python3.9/site-packages/tweetnlp/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feature_extraction as fe\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a4c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>modi took revenge you bcos you are advani team...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>really doubt any organisation allowed function...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>⃣ already possession this capabilities but thi...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>although nasa doesnt have space suits for wome...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>modi one side and the other side its like comp...</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text origin     model\n",
       "0       Our rules are the most robust, the president c...     AI  chat gpt\n",
       "1       ...associating outsiders with danger and extre...     AI  chat gpt\n",
       "2       ...alarms were raised in the initial stages of...     AI  chat gpt\n",
       "3       [The opposition] utilizes every tool to questi...     AI  chat gpt\n",
       "4       [The governor's] fixation with health protocol...     AI  chat gpt\n",
       "...                                                   ...    ...       ...\n",
       "199995  modi took revenge you bcos you are advani team...  human       NaN\n",
       "199996  really doubt any organisation allowed function...  human       NaN\n",
       "199997  ⃣ already possession this capabilities but thi...  human       NaN\n",
       "199998  although nasa doesnt have space suits for wome...  human       NaN\n",
       "199999  modi one side and the other side its like comp...  human       NaN\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt = pd.read_csv('df_pronto/df_gpt.csv')\n",
    "df_human = pd.read_csv('df_pronto/df_human.csv')\n",
    "#df_gpt = df_gpt.iloc[:10000]\n",
    "df_human = df_human.iloc[:100000]\n",
    "df_test = pd.concat([df_gpt, df_human], ignore_index=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672e2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Selecionar a coluna de textos\n",
    "# (substitua 'texto' pelo nome real da coluna com o conteúdo)\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "badf2411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [37:08<00:00, 89.73it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar resultados\n",
    "lexical_features = []\n",
    "\n",
    "for texto in tqdm(textos):\n",
    "    # Tokeniza o texto usando Twokenizer\n",
    "    words = fe.preprocess_text(texto)  # já retorna tokens em lowercase\n",
    "    \n",
    "    # Extrai features léxicas\n",
    "    features = fe.extract_lexical_features(texto, words)\n",
    "    lexical_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8bf10cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.882353                  17                    15   \n",
       "1                  1.000000                  18                    18   \n",
       "2                  0.937500                  32                    30   \n",
       "3                  0.727273                  22                    16   \n",
       "4                  0.923077                  13                    12   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 5.294118                      6.442907   \n",
       "1                 6.000000                      9.000000   \n",
       "2                 4.968750                      4.655273   \n",
       "3                 5.409091                      8.787190   \n",
       "4                 6.153846                      7.207101   \n",
       "\n",
       "   lexical_stopword_ratio  \n",
       "0                0.411765  \n",
       "1                0.277778  \n",
       "2                0.437500  \n",
       "3                0.500000  \n",
       "4                0.384615  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lexical = pd.DataFrame(lexical_features)\n",
    "df_lexical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b23d0af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.882353                  17                    15   \n",
       "1                  1.000000                  18                    18   \n",
       "2                  0.937500                  32                    30   \n",
       "3                  0.727273                  22                    16   \n",
       "4                  0.923077                  13                    12   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 5.294118                      6.442907   \n",
       "1                 6.000000                      9.000000   \n",
       "2                 4.968750                      4.655273   \n",
       "3                 5.409091                      8.787190   \n",
       "4                 6.153846                      7.207101   \n",
       "\n",
       "   lexical_stopword_ratio  \n",
       "0                0.411765  \n",
       "1                0.277778  \n",
       "2                0.437500  \n",
       "3                0.500000  \n",
       "4                0.384615  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([df_test, df_lexical], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9ba12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizando textos: 100%|██████████| 200000/200000 [36:52<00:00, 90.38it/s] \n",
      "Extraindo features estilísticas: 100%|██████████| 200000/200000 [00:09<00:00, 21115.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stylistic_random_uppercase</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>stylistic_question_density</th>\n",
       "      <th>stylistic_ellipsis_count</th>\n",
       "      <th>stylistic_emoji_density</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "      <th>stylistic_capitalization_inconsistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stylistic_random_uppercase  stylistic_repeated_chars  \\\n",
       "0                           0                         0   \n",
       "1                           0                         1   \n",
       "2                           0                         1   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "\n",
       "   stylistic_repeated_words  stylistic_exclamation_density  \\\n",
       "0                         0                            0.0   \n",
       "1                         0                            0.0   \n",
       "2                         0                            0.0   \n",
       "3                         0                            0.0   \n",
       "4                         0                            0.0   \n",
       "\n",
       "   stylistic_question_density  stylistic_ellipsis_count  \\\n",
       "0                         0.0                         0   \n",
       "1                         0.0                         1   \n",
       "2                         0.0                         1   \n",
       "3                         0.0                         0   \n",
       "4                         0.0                         0   \n",
       "\n",
       "   stylistic_emoji_density  stylistic_emoticon_density  \\\n",
       "0                      0.0                         0.0   \n",
       "1                      0.0                         0.0   \n",
       "2                      0.0                         0.0   \n",
       "3                      0.0                         0.0   \n",
       "4                      0.0                         0.0   \n",
       "\n",
       "   stylistic_capitalization_inconsistency  \n",
       "0                                0.000000  \n",
       "1                                0.055556  \n",
       "2                                0.000000  \n",
       "3                                0.000000  \n",
       "4                                0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokeniza todos os textos primeiro\n",
    "processed_texts = [fe.preprocess_text(texto) for texto in tqdm(textos, desc=\"Tokenizando textos\")]\n",
    "stylistic_features = [\n",
    "    fe.extract_stylistic_features(textos[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(stylistic_features)\n",
    "df_syntactic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cabf0795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>stylistic_random_uppercase</th>\n",
       "      <th>stylistic_repeated_chars</th>\n",
       "      <th>stylistic_repeated_words</th>\n",
       "      <th>stylistic_exclamation_density</th>\n",
       "      <th>stylistic_question_density</th>\n",
       "      <th>stylistic_ellipsis_count</th>\n",
       "      <th>stylistic_emoji_density</th>\n",
       "      <th>stylistic_emoticon_density</th>\n",
       "      <th>stylistic_capitalization_inconsistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.882353                  17                    15   \n",
       "1                  1.000000                  18                    18   \n",
       "2                  0.937500                  32                    30   \n",
       "3                  0.727273                  22                    16   \n",
       "4                  0.923077                  13                    12   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 5.294118                      6.442907   \n",
       "1                 6.000000                      9.000000   \n",
       "2                 4.968750                      4.655273   \n",
       "3                 5.409091                      8.787190   \n",
       "4                 6.153846                      7.207101   \n",
       "\n",
       "   lexical_stopword_ratio  stylistic_random_uppercase  \\\n",
       "0                0.411765                           0   \n",
       "1                0.277778                           0   \n",
       "2                0.437500                           0   \n",
       "3                0.500000                           0   \n",
       "4                0.384615                           0   \n",
       "\n",
       "   stylistic_repeated_chars  stylistic_repeated_words  \\\n",
       "0                         0                         0   \n",
       "1                         1                         0   \n",
       "2                         1                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   stylistic_exclamation_density  stylistic_question_density  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   stylistic_ellipsis_count  stylistic_emoji_density  \\\n",
       "0                         0                      0.0   \n",
       "1                         1                      0.0   \n",
       "2                         1                      0.0   \n",
       "3                         0                      0.0   \n",
       "4                         0                      0.0   \n",
       "\n",
       "   stylistic_emoticon_density  stylistic_capitalization_inconsistency  \n",
       "0                         0.0                                0.000000  \n",
       "1                         0.0                                0.055556  \n",
       "2                         0.0                                0.000000  \n",
       "3                         0.0                                0.000000  \n",
       "4                         0.0                                0.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([df_test, df_syntactic], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37ca4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('df_pronto/df_2pronto.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd55e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[our, rules, are, the, most, robust, the, pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[associating, outsiders, with, danger, and, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[alarms, were, raised, in, the, initial, stage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, opposition, utilizes, every, tool, to, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, governor, 's, fixation, with, health, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>[modi, took, revenge, you, bcos, you, are, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>[really, doubt, any, organisation, allowed, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>[⃣, already, possession, this, capabilities, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>[although, nasa, does, nt, have, space, suits,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>[modi, one, side, and, the, other, side, its, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           processed_text\n",
       "0       [our, rules, are, the, most, robust, the, pres...\n",
       "1       [associating, outsiders, with, danger, and, ex...\n",
       "2       [alarms, were, raised, in, the, initial, stage...\n",
       "3       [the, opposition, utilizes, every, tool, to, q...\n",
       "4       [the, governor, 's, fixation, with, health, pr...\n",
       "...                                                   ...\n",
       "199995  [modi, took, revenge, you, bcos, you, are, adv...\n",
       "199996  [really, doubt, any, organisation, allowed, fu...\n",
       "199997  [⃣, already, possession, this, capabilities, b...\n",
       "199998  [although, nasa, does, nt, have, space, suits,...\n",
       "199999  [modi, one, side, and, the, other, side, its, ...\n",
       "\n",
       "[200000 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_texts = pd.DataFrame({'processed_text': processed_texts})\n",
    "df_processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adc8a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_texts.to_csv('df_pronto/df_tokens_gpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_764190/2166943379.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv('df_pronto/df_2pronto.csv')\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('df_pronto/df_2pronto.csv')\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918e9a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_764190/763372016.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv('df_pronto/df_2pronto.csv')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_pronto/df_2pronto.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m textos \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 8\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m [fe\u001b[38;5;241m.\u001b[39mpreprocess_text_A(texto) \u001b[38;5;28;01mfor\u001b[39;00m texto \u001b[38;5;129;01min\u001b[39;00m tqdm(textos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizando textos\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      9\u001b[0m stylistic_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     fe\u001b[38;5;241m.\u001b[39mextract_stylistic_featuresA(textos[i], \u001b[38;5;28mlen\u001b[39m(processed_texts[i]))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(textos)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo features estilísticas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m df_syntactic \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stylistic_features)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_pronto/df_2pronto.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m textos \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 8\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mfe\u001b[49m\u001b[38;5;241m.\u001b[39mpreprocess_text_A(texto) \u001b[38;5;28;01mfor\u001b[39;00m texto \u001b[38;5;129;01min\u001b[39;00m tqdm(textos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizando textos\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      9\u001b[0m stylistic_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     fe\u001b[38;5;241m.\u001b[39mextract_stylistic_featuresA(textos[i], \u001b[38;5;28mlen\u001b[39m(processed_texts[i]))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(textos)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo features estilísticas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m df_syntactic \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stylistic_features)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fe' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokeniza todos os textos primeiro\n",
    "# FALTA FAZER ESSEEEEEEEEEE\n",
    "#\n",
    "#\n",
    "#\n",
    "df_test = pd.read_csv('df_pronto/df_2pronto.csv')\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()\n",
    "processed_texts = [fe.preprocess_text_A(texto) for texto in tqdm(textos, desc=\"Tokenizando textos\")]\n",
    "stylistic_features = [\n",
    "    fe.extract_stylistic_featuresA(textos[i], len(processed_texts[i]))\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estilísticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(stylistic_features)\n",
    "df_syntactic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99571eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702265/2159682550.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv('df_pronto/df_3pronto.csv')\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('df_pronto/df_2pronto.csv')\n",
    "df_processed_texts = pd.read_csv('df_pronto/df_tokens_gpt.csv')\n",
    "processed_texts = df_processed_texts['processed_text'].apply(ast.literal_eval).tolist()\n",
    "textos = df_test[\"text\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04834061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m syntactic_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     fe\u001b[38;5;241m.\u001b[39mextract_syntactic_features(processed_texts[i])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(textos)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo features sintáticas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m df_syntactic \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(syntactic_features)\n\u001b[1;32m      6\u001b[0m df_syntactic\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m syntactic_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mfe\u001b[49m\u001b[38;5;241m.\u001b[39mextract_syntactic_features(processed_texts[i])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(textos)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo features sintáticas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m df_syntactic \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(syntactic_features)\n\u001b[1;32m      6\u001b[0m df_syntactic\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fe' is not defined"
     ]
    }
   ],
   "source": [
    "syntactic_features = [\n",
    "    fe.extract_syntactic_features(processed_texts[i])\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features sintáticas\")\n",
    "]\n",
    "df_syntactic = pd.DataFrame(syntactic_features)\n",
    "df_syntactic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo Features Estruturais: 100%|██████████| 200000/200000 [07:43<00:00, 431.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structural_has_url</th>\n",
       "      <th>structural_has_mention</th>\n",
       "      <th>structural_has_hashtag</th>\n",
       "      <th>structural_is_retweet</th>\n",
       "      <th>structural_url_density</th>\n",
       "      <th>structural_mention_density</th>\n",
       "      <th>structural_hashtag_density</th>\n",
       "      <th>structural_extra_spaces</th>\n",
       "      <th>structural_temporal_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   structural_has_url  structural_has_mention  structural_has_hashtag  \\\n",
       "0                   0                       0                       0   \n",
       "1                   0                       0                       0   \n",
       "2                   0                       0                       0   \n",
       "3                   0                       0                       0   \n",
       "4                   0                       0                       0   \n",
       "\n",
       "   structural_is_retweet  structural_url_density  structural_mention_density  \\\n",
       "0                      0                     0.0                         0.0   \n",
       "1                      0                     0.0                         0.0   \n",
       "2                      0                     0.0                         0.0   \n",
       "3                      0                     0.0                         0.0   \n",
       "4                      0                     0.0                         0.0   \n",
       "\n",
       "   structural_hashtag_density  structural_extra_spaces  \\\n",
       "0                         0.0                      0.0   \n",
       "1                         0.0                      0.0   \n",
       "2                         0.0                      0.0   \n",
       "3                         0.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "\n",
       "   structural_temporal_markers  \n",
       "0                          0.0  \n",
       "1                          0.0  \n",
       "2                          0.0  \n",
       "3                          0.0  \n",
       "4                          0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_features = [\n",
    "    fe.extract_structural_features(textos[i])\n",
    "    for i in tqdm(range(len(textos)), desc=\"Extraindo features estruturais\")\n",
    "]\n",
    "df_structural = pd.DataFrame(structural_features)\n",
    "df_structural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15375807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test, df_structural], axis=1)\n",
    "df_test.to_csv('df_pronto/df_3pronto.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4f12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:31<00:00, 217.81it/s]it/s]\n",
      "100%|██████████| 20000/20000 [01:28<00:00, 226.05it/s]8, 96.45s/it]\n",
      "100%|██████████| 20000/20000 [01:28<00:00, 226.83it/s]8, 93.60s/it]\n",
      "100%|██████████| 20000/20000 [01:25<00:00, 234.39it/s]8, 92.62s/it]\n",
      "100%|██████████| 20000/20000 [01:24<00:00, 235.63it/s]5, 90.95s/it]\n",
      "100%|██████████| 20000/20000 [00:55<00:00, 363.23it/s]9, 89.89s/it]\n",
      "100%|██████████| 20000/20000 [00:54<00:00, 368.40it/s]5, 78.85s/it]\n",
      "100%|██████████| 20000/20000 [01:04<00:00, 310.68it/s]4, 71.48s/it]\n",
      "100%|██████████| 20000/20000 [01:07<00:00, 296.15it/s]0, 70.23s/it]\n",
      "100%|██████████| 20000/20000 [01:01<00:00, 327.00it/s]0, 70.48s/it]\n",
      "Processando N-grams: 100%|██████████| 10/10 [12:52<00:00, 77.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2023</th>\n",
       "      <th>about</th>\n",
       "      <th>about climate</th>\n",
       "      <th>about climate change</th>\n",
       "      <th>about how</th>\n",
       "      <th>about the</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>the scientists</th>\n",
       "      <th>took</th>\n",
       "      <th>towards</th>\n",
       "      <th>updates</th>\n",
       "      <th>vision</th>\n",
       "      <th>waiting for</th>\n",
       "      <th>weapon</th>\n",
       "      <th>with important</th>\n",
       "      <th>with important message</th>\n",
       "      <th>you modi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  2023  about  about climate  about climate change  about how  about the  \\\n",
       "0   0     0      0              0                     0          0          0   \n",
       "1   0     0      0              0                     0          0          0   \n",
       "2   0     0      0              0                     0          0          0   \n",
       "3   0     0      0              0                     0          0          0   \n",
       "4   0     0      0              0                     0          0          0   \n",
       "\n",
       "   across  act  action  ...  the scientists  took  towards  updates  vision  \\\n",
       "0       0    0       0  ...             NaN   NaN      NaN      NaN     NaN   \n",
       "1       0    0       0  ...             NaN   NaN      NaN      NaN     NaN   \n",
       "2       0    0       0  ...             NaN   NaN      NaN      NaN     NaN   \n",
       "3       0    0       0  ...             NaN   NaN      NaN      NaN     NaN   \n",
       "4       0    0       0  ...             NaN   NaN      NaN      NaN     NaN   \n",
       "\n",
       "   waiting for  weapon  with important  with important message  you modi  \n",
       "0          NaN     NaN             NaN                     NaN       NaN  \n",
       "1          NaN     NaN             NaN                     NaN       NaN  \n",
       "2          NaN     NaN             NaN                     NaN       NaN  \n",
       "3          NaN     NaN             NaN                     NaN       NaN  \n",
       "4          NaN     NaN             NaN                     NaN       NaN  \n",
       "\n",
       "[5 rows x 1773 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de uso básico\n",
    "ngrams_df = fe.extract_large_ngrams(textos, max_features=500, batch_size=20000)\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193a3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar os índices de ambos os DataFrames\n",
    "df_test_reset = df_test.reset_index(drop=True)\n",
    "ngrams_df_reset = ngrams_df.reset_index(drop=True)\n",
    "\n",
    "# Agora concatenar\n",
    "df_combined = pd.concat([df_test_reset, ngrams_df_reset], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67511cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>model</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>the scientists</th>\n",
       "      <th>took</th>\n",
       "      <th>towards</th>\n",
       "      <th>updates</th>\n",
       "      <th>vision</th>\n",
       "      <th>waiting for</th>\n",
       "      <th>weapon</th>\n",
       "      <th>with important</th>\n",
       "      <th>with important message</th>\n",
       "      <th>you modi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>2.032114</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1.933809</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.089713</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.894287</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>AI</td>\n",
       "      <td>chat gpt</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.927392</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text origin     model  \\\n",
       "0  Our rules are the most robust, the president c...     AI  chat gpt   \n",
       "1  ...associating outsiders with danger and extre...     AI  chat gpt   \n",
       "2  ...alarms were raised in the initial stages of...     AI  chat gpt   \n",
       "3  [The opposition] utilizes every tool to questi...     AI  chat gpt   \n",
       "4  [The governor's] fixation with health protocol...     AI  chat gpt   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.882353                  17                    15   \n",
       "1                  1.000000                  18                    18   \n",
       "2                  0.937500                  32                    30   \n",
       "3                  0.727273                  22                    16   \n",
       "4                  0.923077                  13                    12   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 5.294118                      6.442907   \n",
       "1                 6.000000                      9.000000   \n",
       "2                 4.968750                      4.655273   \n",
       "3                 5.409091                      8.787190   \n",
       "4                 6.153846                      7.207101   \n",
       "\n",
       "   lexical_stopword_ratio  syntactic_pos_tag_entropy  ...  the scientists  \\\n",
       "0                0.411765                   2.032114  ...             NaN   \n",
       "1                0.277778                   1.933809  ...             NaN   \n",
       "2                0.437500                   2.089713  ...             NaN   \n",
       "3                0.500000                   1.894287  ...             NaN   \n",
       "4                0.384615                   1.927392  ...             NaN   \n",
       "\n",
       "   took  towards  updates  vision  waiting for  weapon  with important  \\\n",
       "0   NaN      NaN      NaN     NaN          NaN     NaN             NaN   \n",
       "1   NaN      NaN      NaN     NaN          NaN     NaN             NaN   \n",
       "2   NaN      NaN      NaN     NaN          NaN     NaN             NaN   \n",
       "3   NaN      NaN      NaN     NaN          NaN     NaN             NaN   \n",
       "4   NaN      NaN      NaN     NaN          NaN     NaN             NaN   \n",
       "\n",
       "   with important message  you modi  \n",
       "0                     NaN       NaN  \n",
       "1                     NaN       NaN  \n",
       "2                     NaN       NaN  \n",
       "3                     NaN       NaN  \n",
       "4                     NaN       NaN  \n",
       "\n",
       "[5 rows x 1806 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9a1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702265/4028979220.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>the scientists</th>\n",
       "      <th>took</th>\n",
       "      <th>towards</th>\n",
       "      <th>updates</th>\n",
       "      <th>vision</th>\n",
       "      <th>waiting for</th>\n",
       "      <th>weapon</th>\n",
       "      <th>with important</th>\n",
       "      <th>with important message</th>\n",
       "      <th>you modi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our rules are the most robust, the president c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5.294118</td>\n",
       "      <td>6.442907</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>2.032114</td>\n",
       "      <td>2.780466</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...associating outsiders with danger and extre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1.933809</td>\n",
       "      <td>2.902002</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...alarms were raised in the initial stages of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>4.655273</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.089713</td>\n",
       "      <td>3.198872</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opposition] utilizes every tool to questi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.409091</td>\n",
       "      <td>8.787190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.894287</td>\n",
       "      <td>2.375897</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The governor's] fixation with health protocol...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7.207101</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.927392</td>\n",
       "      <td>2.523211</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  origin  \\\n",
       "0  Our rules are the most robust, the president c...       1   \n",
       "1  ...associating outsiders with danger and extre...       1   \n",
       "2  ...alarms were raised in the initial stages of...       1   \n",
       "3  [The opposition] utilizes every tool to questi...       1   \n",
       "4  [The governor's] fixation with health protocol...       1   \n",
       "\n",
       "   lexical_type_token_ratio  lexical_word_count  lexical_unique_words  \\\n",
       "0                  0.882353                  17                    15   \n",
       "1                  1.000000                  18                    18   \n",
       "2                  0.937500                  32                    30   \n",
       "3                  0.727273                  22                    16   \n",
       "4                  0.923077                  13                    12   \n",
       "\n",
       "   lexical_avg_word_length  lexical_word_length_variance  \\\n",
       "0                 5.294118                      6.442907   \n",
       "1                 6.000000                      9.000000   \n",
       "2                 4.968750                      4.655273   \n",
       "3                 5.409091                      8.787190   \n",
       "4                 6.153846                      7.207101   \n",
       "\n",
       "   lexical_stopword_ratio  syntactic_pos_tag_entropy  \\\n",
       "0                0.411765                   2.032114   \n",
       "1                0.277778                   1.933809   \n",
       "2                0.437500                   2.089713   \n",
       "3                0.500000                   1.894287   \n",
       "4                0.384615                   1.927392   \n",
       "\n",
       "   syntactic_pos_bigram_entropy  ...  the scientists  took  towards  updates  \\\n",
       "0                      2.780466  ...             NaN   NaN      NaN      NaN   \n",
       "1                      2.902002  ...             NaN   NaN      NaN      NaN   \n",
       "2                      3.198872  ...             NaN   NaN      NaN      NaN   \n",
       "3                      2.375897  ...             NaN   NaN      NaN      NaN   \n",
       "4                      2.523211  ...             NaN   NaN      NaN      NaN   \n",
       "\n",
       "   vision  waiting for  weapon  with important  with important message  \\\n",
       "0     NaN          NaN     NaN             NaN                     NaN   \n",
       "1     NaN          NaN     NaN             NaN                     NaN   \n",
       "2     NaN          NaN     NaN             NaN                     NaN   \n",
       "3     NaN          NaN     NaN             NaN                     NaN   \n",
       "4     NaN          NaN     NaN             NaN                     NaN   \n",
       "\n",
       "   you modi  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "\n",
       "[5 rows x 1805 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove a coluna 'model'\n",
    "#df_combined = df_combined.drop(columns=['model'])\n",
    "\n",
    "# Troca IA → 1 e human → 0\n",
    "df_combined['origin'] = df_combined['origin'].replace({'AI': 1, 'human': 0})\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e5e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702265/91803334.py:1: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  df_combined = df_combined.apply(lambda x: x.sparse.to_dense() if pd.api.types.is_sparse(x) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>lexical_type_token_ratio</th>\n",
       "      <th>lexical_word_count</th>\n",
       "      <th>lexical_unique_words</th>\n",
       "      <th>lexical_avg_word_length</th>\n",
       "      <th>lexical_word_length_variance</th>\n",
       "      <th>lexical_stopword_ratio</th>\n",
       "      <th>syntactic_pos_tag_entropy</th>\n",
       "      <th>syntactic_pos_bigram_entropy</th>\n",
       "      <th>syntactic_avg_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>the scientists</th>\n",
       "      <th>took</th>\n",
       "      <th>towards</th>\n",
       "      <th>updates</th>\n",
       "      <th>vision</th>\n",
       "      <th>waiting for</th>\n",
       "      <th>weapon</th>\n",
       "      <th>with important</th>\n",
       "      <th>with important message</th>\n",
       "      <th>you modi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.0000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.939904</td>\n",
       "      <td>20.395885</td>\n",
       "      <td>18.869735</td>\n",
       "      <td>5.145131</td>\n",
       "      <td>10.948684</td>\n",
       "      <td>0.421463</td>\n",
       "      <td>2.029532</td>\n",
       "      <td>2.716272</td>\n",
       "      <td>14.310194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.065652</td>\n",
       "      <td>9.664566</td>\n",
       "      <td>8.529665</td>\n",
       "      <td>1.318695</td>\n",
       "      <td>13.854440</td>\n",
       "      <td>0.159344</td>\n",
       "      <td>0.364268</td>\n",
       "      <td>0.579060</td>\n",
       "      <td>8.800167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086305</td>\n",
       "      <td>0.097054</td>\n",
       "      <td>0.086899</td>\n",
       "      <td>0.095486</td>\n",
       "      <td>0.104991</td>\n",
       "      <td>0.093434</td>\n",
       "      <td>0.116315</td>\n",
       "      <td>0.082499</td>\n",
       "      <td>0.0822</td>\n",
       "      <td>0.080974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>4.093333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.848011</td>\n",
       "      <td>2.458311</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.944444</td>\n",
       "      <td>6.616071</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.117277</td>\n",
       "      <td>2.886165</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.608696</td>\n",
       "      <td>12.290000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>2.290368</td>\n",
       "      <td>3.130860</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>2348.913580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.720884</td>\n",
       "      <td>3.912701</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              origin  lexical_type_token_ratio  lexical_word_count  \\\n",
       "count  200000.000000             200000.000000       200000.000000   \n",
       "mean        0.500000                  0.939904           20.395885   \n",
       "std         0.500001                  0.065652            9.664566   \n",
       "min         0.000000                  0.076923            1.000000   \n",
       "25%         0.000000                  0.903226           12.000000   \n",
       "50%         0.500000                  0.950000           20.000000   \n",
       "75%         1.000000                  1.000000           28.000000   \n",
       "max         1.000000                  1.000000          107.000000   \n",
       "\n",
       "       lexical_unique_words  lexical_avg_word_length  \\\n",
       "count         200000.000000            200000.000000   \n",
       "mean              18.869735                 5.145131   \n",
       "std                8.529665                 1.318695   \n",
       "min                1.000000                 1.000000   \n",
       "25%               12.000000                 4.375000   \n",
       "50%               19.000000                 4.944444   \n",
       "75%               26.000000                 5.608696   \n",
       "max               76.000000                76.000000   \n",
       "\n",
       "       lexical_word_length_variance  lexical_stopword_ratio  \\\n",
       "count                 200000.000000           200000.000000   \n",
       "mean                      10.948684                0.421463   \n",
       "std                       13.854440                0.159344   \n",
       "min                        0.000000                0.000000   \n",
       "25%                        4.093333                0.333333   \n",
       "50%                        6.616071                0.437500   \n",
       "75%                       12.290000                0.533333   \n",
       "max                     2348.913580                1.000000   \n",
       "\n",
       "       syntactic_pos_tag_entropy  syntactic_pos_bigram_entropy  \\\n",
       "count              200000.000000                 200000.000000   \n",
       "mean                    2.029532                      2.716272   \n",
       "std                     0.364268                      0.579060   \n",
       "min                    -0.000000                     -0.000000   \n",
       "25%                     1.848011                      2.458311   \n",
       "50%                     2.117277                      2.886165   \n",
       "75%                     2.290368                      3.130860   \n",
       "max                     2.720884                      3.912701   \n",
       "\n",
       "       syntactic_avg_sentence_length  ...  the scientists          took  \\\n",
       "count                  200000.000000  ...    20000.000000  20000.000000   \n",
       "mean                       14.310194  ...        0.007200      0.009000   \n",
       "std                         8.800167  ...        0.086305      0.097054   \n",
       "min                         0.500000  ...        0.000000      0.000000   \n",
       "25%                         8.000000  ...        0.000000      0.000000   \n",
       "50%                        12.000000  ...        0.000000      0.000000   \n",
       "75%                        17.000000  ...        0.000000      0.000000   \n",
       "max                       107.000000  ...        2.000000      2.000000   \n",
       "\n",
       "            towards       updates        vision   waiting for        weapon  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       0.007000      0.009100      0.008800      0.008400      0.013100   \n",
       "std        0.086899      0.095486      0.104991      0.093434      0.116315   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        2.000000      2.000000      3.000000      2.000000      2.000000   \n",
       "\n",
       "       with important  with important message      you modi  \n",
       "count    20000.000000              20000.0000  20000.000000  \n",
       "mean         0.006650                  0.0066      0.006600  \n",
       "std          0.082499                  0.0822      0.080974  \n",
       "min          0.000000                  0.0000      0.000000  \n",
       "25%          0.000000                  0.0000      0.000000  \n",
       "50%          0.000000                  0.0000      0.000000  \n",
       "75%          0.000000                  0.0000      0.000000  \n",
       "max          2.000000                  2.0000      1.000000  \n",
       "\n",
       "[8 rows x 1804 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = df_combined.apply(lambda x: x.sparse.to_dense() if pd.api.types.is_sparse(x) else x)\n",
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"df_pronto/df_gpt_com_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meu_ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
